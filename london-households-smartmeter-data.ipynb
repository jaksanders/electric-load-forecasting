{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jaksanders/electric-load-forecasting-with-deep-learning?scriptVersionId=145103421\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Short-term residential load forecasting with Deep Learning\n\nLondon Households SmartMeter Data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-02T14:28:39.787361Z","iopub.execute_input":"2023-10-02T14:28:39.787845Z","iopub.status.idle":"2023-10-02T14:28:39.824611Z","shell.execute_reply.started":"2023-10-02T14:28:39.787802Z","shell.execute_reply":"2023-10-02T14:28:39.823684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nimport IPython\nimport IPython.display\nimport glob\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nrandomState = 42 # tip of the cap to Douglas Adams\n#!pip install -U pip\n#!pip install -U setuptools wheel\n\n#!pip install autogluon\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T14:28:46.783923Z","iopub.execute_input":"2023-10-02T14:28:46.784347Z","iopub.status.idle":"2023-10-02T14:28:53.012592Z","shell.execute_reply.started":"2023-10-02T14:28:46.784317Z","shell.execute_reply":"2023-10-02T14:28:53.011665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creat two load forecasts...\n1) half-hourly load forecast for next 24 hours\n2) peak half-hour in the next 24 hours\n\nValue...\n* For electric network operator, minimize the amount of spinning reserve\n\nData...\n* residential smart meter usage data\n* weather data\n* weather forecast data","metadata":{}},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"markdown","source":"## Load half-hourly electric usage data\n...for ~5k smart meters in London\n[SmartMeter Energy Consumption Data in London Households](https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households)","metadata":{}},{"cell_type":"code","source":"import time\n# load half-hourly electric usage data\n# takes about four minutes, need to find somerthing faster like Dask?\n# https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households\nd = pd.read_csv('/kaggle/input/small-lcl-data/LCL-June2015v2_99.csv', parse_dates=[\"DateTime\"])\n\n# Get CSV files list from a folder\npath = '/kaggle/input/smart-meters-in-london/halfhourly_dataset/halfhourly_dataset'\ncsv_files = glob.glob(path + \"/*.csv\")\n\n# Read each CSV file into DataFrame\n# This creates a list of dataframes\nstart_time = time.time()\ndf_list = (pd.read_csv(file, parse_dates=[\"tstp\"]) for file in csv_files)\nprint('%s seconds' % (time.time() - start_time))\n\n# Concatenate all DataFrames\nstart_time = time.time()\nd = pd.concat(df_list, ignore_index=True)\nprint('%s seconds' % (time.time() - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load hourly weather data","metadata":{}},{"cell_type":"code","source":"# load hourly weather data\n# https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households\nweatherData = pd.read_csv('/kaggle/input/smart-meters-in-london/weather_hourly_darksky.csv', parse_dates=[\"time\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weatherData.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weatherData.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data pre-processing and cleaning","metadata":{}},{"cell_type":"markdown","source":"## Weather data: convert text attributes to string datatype","metadata":{}},{"cell_type":"code","source":"weatherData = weatherData.astype({'precipType':'string', 'icon':'string', 'summary':'string'})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas_profiling import ProfileReport\n\nprofile = ProfileReport(weatherData, tsmode=True, sortby=\"time\")\nprofile.to_file('weatherData profile_report.html')\n# profile\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Identify and remove weather records not exactly on the hour","metadata":{}},{"cell_type":"code","source":"# inspect and remove records not exactly on the hour\noffRecs = weatherData.query(\"time.dt.minute != 0 or time.dt.second != 0\")\nprint('Records not exactly on the half-hour:\\n ', offRecs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Upsample weather data to match half-houly sampling rate of load data","metadata":{}},{"cell_type":"code","source":"# select weather data features of interest\nweatherUpsample = weatherData[['time','temperature', 'dewPoint', 'pressure', 'humidity']].copy()\nweatherUpsample = weatherUpsample.sort_values(by=['time'])\nprint(weatherUpsample.info())\nprint(weatherUpsample.describe())\nprint(weatherUpsample)\n\nweatherUpsample = weatherUpsample.set_index('time')\nweatherUpsample.index.rename('time', inplace=True)\n\nstart_time = time.time()\n\nweatherUpsample = weatherUpsample.resample('30Min').mean()\n\n# upsample \nweatherUpsample['temperature'] = weatherUpsample['temperature'].interpolate()\nweatherUpsample['dewPoint'] = weatherUpsample['dewPoint'].interpolate()\nweatherUpsample['pressure'] = weatherUpsample['pressure'].interpolate()\nweatherUpsample['humidity'] = weatherUpsample['humidity'].interpolate()\n\nprint('%s seconds' % (time.time() - start_time))\n\nweatherUpsample = weatherUpsample.reset_index(names='DateTime')\nprint(weatherUpsample.info())\nprint(weatherUpsample.describe())\nprint(weatherUpsample)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weatherUpsample.to_csv('/kaggle/working/WeatherDataFinal.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weatherUpsample = pd.read_csv('/kaggle/working/WeatherDataFinal.csv', parse_dates=[\"DateTime\"])\nweatherUpsample","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:54:51.414567Z","iopub.execute_input":"2023-10-02T19:54:51.415335Z","iopub.status.idle":"2023-10-02T19:54:51.504113Z","shell.execute_reply.started":"2023-10-02T19:54:51.415286Z","shell.execute_reply":"2023-10-02T19:54:51.503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# utility function to nicely format variable names and memory they are consuming\nimport sys\ndef sizeof_fmt(num, suffix='B'):\n    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert smart meter usage datatype to float","metadata":{}},{"cell_type":"code","source":"# ~1 minute\nstart_time = time.time()\nd.iloc[:, 2] = pd.to_numeric(d.iloc[:, 2], errors='coerce')\nprint('%s seconds' % (time.time() - start_time))\n\n# rename usage column for easier reference\nd.rename(columns={d.columns[2]: 'KWHperHH'}, inplace=True)\nd.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set timestamp as the index\nstart_time = time.time()\nd.set_index('tstp')\nprint('%s seconds' % (time.time() - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Identify and handle duplicates in the smart meter data","metadata":{}},{"cell_type":"code","source":"# about 1.5 minutes\nstart_time = time.time()\ndupes = d[d.duplicated()]\nprint('dupes', dupes)\nprint('dupes.index', dupes.index)\nd.drop(index=dupes.index, inplace=True)\nprint('%s seconds' % (time.time() - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set index for the usage data to the timestamp column.  Is this necessary?  Can't remember why\nstart_time = time.time()\nd.set_index('tstp')\nd.info()\nprint('%s seconds' % (time.time() - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check what is gobbling RAM\nfor name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(\n                          locals().items())), key= lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize smart meter dataset to analyze for quality, completenes and othe insights","metadata":{}},{"cell_type":"code","source":"import seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## grab a random sample of 2% of meters for visualization and analysis","metadata":{}},{"cell_type":"code","source":"rng = np.random.default_rng(randomState)\n# random_state = np.random.RandomState(randomState)\nsampleMeters = rng.choice(d.LCLid.unique(), size=int(len(d.LCLid.unique())*0.02), replace=False)\n# sampleMeters = np.random.choice(d.LCLid.unique(), size=int(len(d.LCLid.unique())*0.02), replace=False, random_state=random_state)\nprint('sampleMeters:\\n', sampleMeters)\nsample = d[d['LCLid'].isin(sampleMeters)]\nprint('sample:\\n', sample)\nprint(sample.describe())\n# print(sample.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Heatmap to visualize meter read coverage and completeness","metadata":{}},{"cell_type":"code","source":"# visualize meter read coverage and completeness\n# using a random sample of 2% of meters\nplt.subplots(figsize=(20,15))\npivot_table = pd.pivot_table(sample, columns='tstp', index='LCLid', values='KWHperHH')\nsns.heatmap(pivot_table)\nplt.savefig('meter data heatmap.png', format='png')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations from Heatmap...\n* several houses start producing load part-way through the period\n    - eg MAC004221, MAC004248\n    \n    \n* several houses stop producing part-way through the period\n    - eg MAC004226, MAC004257\n    \n\n* most houses have at least one \"gap\" in their data (visible as white lines)\n\n\n* several houses stand out as having significantly higher average load than others\n    - eg MAC004225, MAC004249","metadata":{}},{"cell_type":"markdown","source":"## identify and remove smart meter readings not exactly on the half-hour","metadata":{}},{"cell_type":"code","source":"# identify and remove records not exactly on the half-hour\n\nstart_time = time.time()\n\noffRecs = d.query(\"tstp.dt.minute not in (0,30) or tstp.dt.second != 0\")\n# aggLoad[\"DateTime\"].dt.hour > 30\nprint('Records not exactly on the half-hour:\\n ', offRecs)\nprint(offRecs.info())\n\n# delete records not exactly on the half-hour\nd.drop(offRecs.index, inplace=True)\n\nprint('%s seconds' % (time.time() - start_time))\n\noffRecs = d.query(\"tstp.dt.minute not in (0,30) or tstp.dt.second != 0\")\nprint('Records not exactly on the half-hour: ', offRecs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check what is gobbling RAM\nfor name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(\n                          locals().items())), key= lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fill gaps in the smart meter data using interpolation\n\nOur heatmap above shows lots of gaps (small white vertical lines), and we'll fill those gaps using interpolation","metadata":{}},{"cell_type":"markdown","source":"### First step of filling these gaps is to create NaN records where records are missing\n\nThen we can fill these gas with interpolation","metadata":{}},{"cell_type":"code","source":"# First step of interpolation is to create NaN records where records are missing\n# about 2 minutes\nd.sort_values(by=['tstp'], inplace=True)\nd.set_index('tstp', inplace=True)\nd.index.rename('tstp', inplace=True)\n\nstart_time = time.time()\n# resample to create NaN records where records are missing\nd = d.groupby('LCLid')\\\n                .resample('30Min')\\\n                .mean()\n\n# fill the gaps with interpolation\nd['KWHperHH'] = d['KWHperHH'].interpolate()\nd.reset_index(inplace=True)\n\nprint('%s seconds' % (time.time() - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check the meter data heatmap to see if gaps have been filled","metadata":{}},{"cell_type":"code","source":"# visualize after interpolating missing values\nd.info()\nsample = d[d['LCLid'].isin(sampleMeters)]\npivot_table = pd.pivot_table(sample, columns='tstp', index='LCLid', values='KWHperHH')\nplt.subplots(figsize=(20,15))\n\nsns.heatmap(pivot_table)\nplt.savefig('meter data heatmap gaps filled.png', format='png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize zeros in the dataset using heatmap\n\nI'm always curious to understand zeros in a dataset, and whether they are legitimate zero values, or indicate a data quality problem.","metadata":{}},{"cell_type":"code","source":"# visualize zeros in the dataset\nstart_time = time.time()\nsample = d[d['LCLid'].isin(sampleMeters)]\nsample['ZeroKWHperHH'] = sample['KWHperHH'] == 0\npivot_table = pd.pivot_table(sample, columns='tstp', index='LCLid', values='ZeroKWHperHH')\nprint('%s seconds' % (time.time() - start_time))\nplt.subplots(figsize=(20,15))\n\nsns.heatmap(pivot_table)\nplt.savefig('meter data heatmap zeros.png', format='png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a snapshot of data\nstart_time = time.time()\nd.to_csv('/kaggle/working/MeterDataFinal.csv',index=False)\nprint('%s seconds' % (time.time() - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Obervation\n\nThere are a handful of households that account all the zero value meter reads: MAC004233, MAC004226, MAC004267","metadata":{}},{"cell_type":"code","source":"# investigate the meters with zero reads\nMAC005127 = sample.query(\"LCLid == 'MAC005127'\")\n\nfig, ax = plt.subplots(4,figsize=(20,9))\n\n# plot whole ~2 years\nax[0].plot(MAC005127.tstp, MAC005127.KWHperHH)\nax[0].plot(MAC005127.tstp, MAC005127.ZeroKWHperHH)\nax[0].set(ylabel='KWH/hh',\n       title='Load from one Household MAC004233 with lots of zero values')\nplt.tick_params(rotation=45)\nax[0].grid()\n\n# zoom in\nax[1].plot(MAC005127.tstp[11000:15000], MAC005127.KWHperHH[11000:15000])\nax[1].plot(MAC005127.tstp[11000:15000], MAC005127.ZeroKWHperHH[11000:15000])\nax[1].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[1].grid()\n\n# zoom in more...\nax[2].plot(MAC005127.tstp[13000:13500], MAC005127.KWHperHH[13000:13500])\nax[2].plot(MAC005127.tstp[13000:13500], MAC005127.ZeroKWHperHH[13000:13500])\nax[2].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[2].grid()\n\n# zoom in to a different part of the series...\nax[3].plot(MAC005127.tstp[25000:25500], MAC005127.KWHperHH[25000:25500])\nax[3].plot(MAC005127.tstp[25000:25500], MAC005127.ZeroKWHperHH[25000:25500])\nax[3].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[3].grid()\n\nfig.savefig(\"MAC005127.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\nThe zeros for MAC004233 seem legit - leaving them in","metadata":{}},{"cell_type":"code","source":"# investigate the meters with zero reads\nMAC002304 = sample.query(\"LCLid == 'MAC002304'\")\nfig, ax = plt.subplots(4,figsize=(20,9))\n\n# plot whole ~2 years\nax[0].plot(MAC002304.tstp, MAC002304.KWHperHH)\nax[0].plot(MAC002304.tstp, MAC002304.ZeroKWHperHH)\nax[0].set(ylabel='KWH/hh',\n       title='Load from one Household MAC002304 with lots of zero values')\nplt.tick_params(rotation=45)\nax[0].grid()\n\n# zoom in\nax[1].plot(MAC002304.tstp[17000:21000], MAC002304.KWHperHH[17000:21000])\nax[1].plot(MAC002304.tstp[17000:21000], MAC002304.ZeroKWHperHH[17000:21000])\nax[1].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[1].grid()\n\n# zoom in more...\nax[2].plot(MAC002304.tstp[19300:19800], MAC002304.KWHperHH[19300:19800])\nax[2].plot(MAC002304.tstp[19300:19800], MAC002304.ZeroKWHperHH[19300:19800])\nax[2].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[2].grid()\n\n# zoom in to a different part of the series...\nax[3].plot(MAC002304.tstp[25000:25500], MAC002304.KWHperHH[25000:25500])\nax[3].plot(MAC002304.tstp[25000:25500], MAC002304.ZeroKWHperHH[25000:25500])\nax[3].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[3].grid()\n\nfig.savefig(\"MAC002304.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation\n\nThe zeros for MAC004233 seem legit - leaving them in\n","metadata":{}},{"cell_type":"code","source":"# visualize and handle outliers\n\n# minumum and maximum timestamp for each house\nprint(d.groupby('LCLid').max().sort_values('tstp'))\nprint(d.groupby('LCLid').min().sort_values('tstp'))\nprint(d.groupby('LCLid').count().sort_values('tstp'))\n\nprint(d.groupby('LCLid').agg(['min', 'max', 'count']))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# which house has the highest peak load?\n\n# which house has the highest total aggregate load?\n\n# how variable / predictable is the timing of the peak load\n\n# how accurate is the next 24 hours forecast profile overall?\n\n# how accurate is the peak load forecast in next 24 hours?\n\n# normalize and standardize\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract one smartmeter for plotting\nsample = d.query(\"LCLid == 'MAC004233'\")\nsample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize load profile for one household meter\nfig, ax = plt.subplots()\nax.plot(sample.iloc[100:4500,1], sample.iloc[100:4500,2])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Load from one Household, June-September 2012')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Load from one Household, June-September 2012.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set index for the sample\nsample.set_index('tstp')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA: Visualize daily average load for each meter and all meters...","metadata":{}},{"cell_type":"code","source":"# calculate average daily load profile for all meters...\n# abut 1.5 minutes\n\nstart_time = time.time()\navgLoadProfile = pd.DataFrame(d.groupby([d['tstp'].dt.hour, d['tstp'].dt.minute]).KWHperHH.mean())\navgLoadProfile = avgLoadProfile.reset_index(names=['hour', 'minute'])\navgLoadProfile['labels'] = pd.to_datetime(avgLoadProfile['hour'].astype(str) + ':' + avgLoadProfile['minute'].astype(str), format='%H:%M').dt.time\nprint('%s seconds' % (time.time() - start_time))\n\n# print(avgLoadProfile.info())\n# print(avgLoadProfile)\n\nfig, ax = plt.subplots(figsize=(10,7))\n\nax.set_xticks(avgLoadProfile.index, avgLoadProfile.labels)\n\nax.set(xlabel='time (HH:MI)', ylabel='KWH/hh',\n       title='Average Household 24 hour load profile')\n\n# calculate average daily load for each meter...\nstart_time = time.time()\navgLoadProfileEachMeter = pd.DataFrame(d.groupby(['LCLid', d['tstp'].dt.hour, d['tstp'].dt.minute]).agg({'KWHperHH': 'mean'}))\navgLoadProfileEachMeter = avgLoadProfileEachMeter.reset_index(names=['LCLid', 'hour', 'minute'])\nprint('%s seconds' % (time.time() - start_time))\n# print(avgLoadProfileEachMeter.info())\n# print(avgLoadProfileEachMeter)\n\n# plot every sample meter\nfor meter in sampleMeters:\n    # print(meter)\n    ax.plot(avgLoadProfileEachMeter.loc[avgLoadProfileEachMeter['LCLid'] == meter].index % 48, \n            avgLoadProfileEachMeter.loc[avgLoadProfileEachMeter['LCLid'] == meter].KWHperHH,\n           color='grey')\n\n# plot the average\nax.plot(avgLoadProfile.index, avgLoadProfile.KWHperHH, linewidth=5)\n\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Avg 24hr Load Profile every meter.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the sum of all loads for each timestamp using `groupby()` and `agg()`\nstart_time = time.time()\n# aggLoad = d.groupby('tstp')['KWHperHH'].agg('sum')\naggLoad = pd.DataFrame(d.groupby('tstp')['KWHperHH'].agg({'sum', 'count'}))\naggLoad.reset_index(inplace=True)\naggLoad.columns = ['tstp', 'AggregateLoad', 'numMeters']\nprint('%s seconds' % (time.time() - start_time))\n\nprint(aggLoad)\nprint(aggLoad.describe())\nprint(aggLoad.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggLoad.sort_values(by=['tstp'], inplace=True)\naggLoad.set_index('tstp', inplace=True)\naggLoad.index.rename('DateTimeIndex', inplace=True)\naggLoad.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggLoad['DateTime'] = aggLoad.index\naggLoad.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inspect and fix records with zero load\n# start with the aggregated records with zero load\nAggZeros = aggLoad.query(\"AggregateLoad == 0\")\nAggZeros\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation: Some of the timestamps are not exactly on the half-hour\nQuestion: How many of the timestamps are not exactly on the half-hour?","metadata":{}},{"cell_type":"code","source":"# inspect and fix records not exactly on the half-hour\noffRecs = aggLoad.query(\"DateTime.dt.minute not in (0,30) or DateTime.dt.second != 0\")\n# aggLoad[\"DateTime\"].dt.hour > 30\nprint('Records not exactly on the half-hour: ', offRecs)\nprint(offRecs.info())\n\n# delete records not exactly on the half-hour\naggLoad = aggLoad.drop(offRecs.index)\n\noffRecs = aggLoad.query(\"DateTime.dt.minute not in (0,30) or DateTime.dt.second != 0\")\nprint('Records not exactly on the half-hour: ', offRecs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the regularity of the observations (time between observations)\n# print(pd.infer_freq(train_data.DateTime))\naggLoad.index.to_series().diff().value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate moving average and stddev\nwindow_size = int(len(aggLoad.AggregateLoad) / 10)\nprint(window_size)\n\naggLoadMovingStdev = aggLoad.AggregateLoad.rolling(window_size).std()\naggLoadMovingStdev.columns = ['MovingStdev']\n# aggLoadMovingStdev.columns.values[0] = 'MovingStdev'\n\naggLoadMovingAvg = aggLoad.AggregateLoad.rolling(window_size).mean()\naggLoadMovingAvg.columns = ['MovingAvg']\n\nprint('aggLoadMovingStdev:\\n', aggLoadMovingStdev)\nprint(aggLoadMovingStdev.info())\nprint('aggLoadMovingAvg:\\n', aggLoadMovingAvg)\nprint(aggLoadMovingAvg.info())\n\n# aggLoad['MovingStdev'] = aggLoad.AggregateLoad.rolling(window_size).std()\n# aggLoad['MovingAvg'] = aggLoad.AggregateLoad.rolling(window_size).mean()\n\n# print('aggLoad.MovingStdev:\\n', aggLoad.MovingStdev)\n# print('aggLoad.MovingAvg:\\n', aggLoad.MovingAvg)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:04:31.95393Z","iopub.execute_input":"2023-10-02T19:04:31.954407Z","iopub.status.idle":"2023-10-02T19:04:31.980286Z","shell.execute_reply.started":"2023-10-02T19:04:31.954373Z","shell.execute_reply":"2023-10-02T19:04:31.979066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(aggLoad)\n\nfig, ax = plt.subplots(figsize=(20,7))\nax.plot(aggLoad.DateTime, aggLoad.AggregateLoad)\n# ax.plot(aggLoad.DateTime, aggLoad.MovingAvg, linewidth=3)\n# ax.plot(aggLoad.DateTime, aggLoad.MovingStdev, linewidth=3)\nax.plot(aggLoad.DateTime, aggLoadMovingAvg, linewidth=3)\nax.plot(aggLoad.DateTime, aggLoadMovingStdev, linewidth=3)\nax.plot(aggLoad.DateTime, aggLoad.numMeters, linewidth=3)\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load 2012-2014')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Aggregate Household load 2012-2014.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:05:53.124327Z","iopub.execute_input":"2023-10-02T19:05:53.125447Z","iopub.status.idle":"2023-10-02T19:05:53.827612Z","shell.execute_reply.started":"2023-10-02T19:05:53.125396Z","shell.execute_reply":"2023-10-02T19:05:53.82651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggLoad.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,7))\nax.plot(aggLoad.DateTime[10000:15000], aggLoad.AggregateLoad[10000:15000])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load June-August 2012')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Aggregate Household load June-August 2012.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,7))\nax.plot(aggLoad.DateTime[12000:13000], aggLoad.AggregateLoad[12000:13000])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Aggregate Household load one month.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,7))\nax.plot(aggLoad.DateTime[12500:12600], aggLoad.AggregateLoad[12500:12600])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load ~two days')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Aggregate Household load two days.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.plot(aggLoad.DateTime[12500:12550], aggLoad.AggregateLoad[12500:12550])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load (one day)')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Aggregate Household load (one day).png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggLoad.to_csv('/kaggle/working/aggLoadDataFinal.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggLoad = pd.read_csv('/kaggle/working/aggLoadDataFinal.csv', parse_dates=[\"DateTime\"])\naggLoad","metadata":{"execution":{"iopub.status.busy":"2023-10-02T14:29:48.304672Z","iopub.execute_input":"2023-10-02T14:29:48.305645Z","iopub.status.idle":"2023-10-02T14:29:48.388295Z","shell.execute_reply.started":"2023-10-02T14:29:48.305599Z","shell.execute_reply":"2023-10-02T14:29:48.387083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add features useful for time series\n!pip install scikit-learn\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import preprocessing","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:03:19.264147Z","iopub.execute_input":"2023-10-02T19:03:19.265067Z","iopub.status.idle":"2023-10-02T19:03:29.440548Z","shell.execute_reply.started":"2023-10-02T19:03:19.26502Z","shell.execute_reply":"2023-10-02T19:03:29.43938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add features useful for time series\nprint(aggLoad.info())\n# day of week\naggLoad[\"dayOfWeek\"] = aggLoad.DateTime.dt.dayofweek\n# day of year\naggLoad[\"dayOfYear\"] = aggLoad.DateTime.dt.dayofyear\n# minute of the day\naggLoad[\"minuteOfDay\"] = (aggLoad.DateTime.dt.hour * 60) + aggLoad.DateTime.dt.minute\n# number of meters\n\nprint(aggLoad.info())","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:13:04.123712Z","iopub.execute_input":"2023-10-02T19:13:04.124358Z","iopub.status.idle":"2023-10-02T19:13:04.169431Z","shell.execute_reply.started":"2023-10-02T19:13:04.124314Z","shell.execute_reply":"2023-10-02T19:13:04.168133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the earliest date when there is the maximum number of meters contributing to the aggregate load\n# will disreard all data beofre this point\nmaxMeters = aggLoad['numMeters'].max()\nprint(maxMeters)\nstartDateTime = aggLoad[aggLoad['numMeters']==maxMeters].DateTime.min()\nprint(startDateTime)\nstartDate = startDateTime.date()\nprint(startDate)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:38:39.153483Z","iopub.execute_input":"2023-10-02T19:38:39.153917Z","iopub.status.idle":"2023-10-02T19:38:39.1634Z","shell.execute_reply.started":"2023-10-02T19:38:39.153885Z","shell.execute_reply":"2023-10-02T19:38:39.162418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indexDate = aggLoad[ aggLoad['DateTime'].dt.date < startDate].index\naggLoad.drop(indexDate , inplace=True)\naggLoad","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:53:29.546543Z","iopub.execute_input":"2023-10-02T19:53:29.547397Z","iopub.status.idle":"2023-10-02T19:53:29.577849Z","shell.execute_reply.started":"2023-10-02T19:53:29.547349Z","shell.execute_reply":"2023-10-02T19:53:29.576734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Join load data and weather data\nmergeData = pd.merge(aggLoad, weatherUpsample, on='DateTime', copy=False)\nprint(mergeData.info())\nmergeData\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:55:18.032754Z","iopub.execute_input":"2023-10-02T19:55:18.033586Z","iopub.status.idle":"2023-10-02T19:55:18.082177Z","shell.execute_reply.started":"2023-10-02T19:55:18.033539Z","shell.execute_reply":"2023-10-02T19:55:18.080947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move first column to the Last\n# df = pd.DataFrame(mergeData)\ndf = mergeData\ntemp_cols=df.columns.tolist()\nnew_cols=temp_cols[1:] + temp_cols[0:1]\nmergeData=df[new_cols]\nprint(mergeData)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:55:38.741137Z","iopub.execute_input":"2023-10-02T19:55:38.742372Z","iopub.status.idle":"2023-10-02T19:55:38.757328Z","shell.execute_reply.started":"2023-10-02T19:55:38.742323Z","shell.execute_reply":"2023-10-02T19:55:38.756338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas_profiling import ProfileReport\n\nprofile = ProfileReport(mergeData, tsmode=True, sortby=\"DateTime\")\nprofile.to_file('mergeData profile_report.html')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoData = mergeData.copy() # keep a copy for the autogluon AutoML","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove the dateTime feature from the dataset\n\nprint(mergeData)\nmergeData = mergeData.drop(columns=['DateTime'])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:56:18.074587Z","iopub.execute_input":"2023-10-02T19:56:18.075609Z","iopub.status.idle":"2023-10-02T19:56:18.090341Z","shell.execute_reply.started":"2023-10-02T19:56:18.075564Z","shell.execute_reply":"2023-10-02T19:56:18.08906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoData.to_csv('/kaggle/working/autoDataFinal.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:56:24.546493Z","iopub.execute_input":"2023-10-02T19:56:24.546902Z","iopub.status.idle":"2023-10-02T19:56:24.856619Z","shell.execute_reply.started":"2023-10-02T19:56:24.54687Z","shell.execute_reply":"2023-10-02T19:56:24.855372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mergeData.to_csv('/kaggle/working/mergeDataFinal.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:56:31.322946Z","iopub.execute_input":"2023-10-02T19:56:31.324401Z","iopub.status.idle":"2023-10-02T19:56:31.470939Z","shell.execute_reply.started":"2023-10-02T19:56:31.324358Z","shell.execute_reply":"2023-10-02T19:56:31.469707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mergeData = pd.read_csv('/kaggle/working/mergeDataFinal.csv')\nmergeData","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:56:35.722317Z","iopub.execute_input":"2023-10-02T19:56:35.722901Z","iopub.status.idle":"2023-10-02T19:56:35.766907Z","shell.execute_reply.started":"2023-10-02T19:56:35.722852Z","shell.execute_reply":"2023-10-02T19:56:35.765784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mergeData.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:56:43.316826Z","iopub.execute_input":"2023-10-02T19:56:43.317699Z","iopub.status.idle":"2023-10-02T19:56:43.332135Z","shell.execute_reply.started":"2023-10-02T19:56:43.317654Z","shell.execute_reply":"2023-10-02T19:56:43.330926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try removing all exogenous features\nmergeData = pd.DataFrame(mergeData.AggregateLoad)\nmergeData.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:56:48.216043Z","iopub.execute_input":"2023-10-02T19:56:48.219562Z","iopub.status.idle":"2023-10-02T19:56:48.236783Z","shell.execute_reply.started":"2023-10-02T19:56:48.219511Z","shell.execute_reply":"2023-10-02T19:56:48.235464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try reducing the size of the dataset to ~1 year\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the time series data into train, test, and validation datasets\ntrain_size = int(len(mergeData) * 0.7)  # 70% for training\nval_size = int(len(mergeData) * 0.2)   # 20% for validation\ntest_size = len(mergeData) - val_size - train_size  # Remaining for testing\n\ntrain_data = mergeData[:train_size].copy()\nval_data = mergeData[train_size:train_size+val_size].copy()\ntest_data = mergeData[train_size+val_size:].copy()\n\nprint('\\ntrain_data.head()\\n', train_data.head())\nprint(train_data.info())\nprint('\\nval_data.head()\\n', val_data.head())\nprint(val_data.info())\nprint('\\ntest_data.head()\\n', test_data.head())\nprint(test_data.info())\n\nnum_features = mergeData.shape[1]","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:32:05.958649Z","iopub.execute_input":"2023-10-03T01:32:05.959142Z","iopub.status.idle":"2023-10-03T01:32:05.993305Z","shell.execute_reply.started":"2023-10-03T01:32:05.959109Z","shell.execute_reply":"2023-10-03T01:32:05.991955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction_plot(testY, test_predict):\n      len_prediction=[x for x in range(len(testY))]\n      plt.figure(figsize=(20,5))\n      plt.plot(len_prediction, testY, marker='.', label=\"actual\")\n      plt.plot(len_prediction, test_predict, 'r', label=\"prediction\")\n      plt.tight_layout()\n      sns.despine(top=True)\n      plt.subplots_adjust(left=0.07)\n      plt.ylabel('KWH per half hour', size=15)\n      plt.xlabel('Time step', size=15)\n      plt.legend(fontsize=15)\n      plt.show();","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:57:15.186975Z","iopub.execute_input":"2023-10-02T19:57:15.187411Z","iopub.status.idle":"2023-10-02T19:57:15.194884Z","shell.execute_reply.started":"2023-10-02T19:57:15.187379Z","shell.execute_reply":"2023-10-02T19:57:15.193729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize the data\ntrain_mean = train_data.mean()\ntrain_std = train_data.std()\n\ntrain_data = (train_data - train_mean) / train_std\nval_data = (val_data - train_mean) / train_std\ntest_data = (test_data - train_mean) / train_std\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:57:18.969618Z","iopub.execute_input":"2023-10-02T19:57:18.970077Z","iopub.status.idle":"2023-10-02T19:57:18.981121Z","shell.execute_reply.started":"2023-10-02T19:57:18.970038Z","shell.execute_reply":"2023-10-02T19:57:18.980165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize distribution of the features\ndf_std = (mergeData - train_mean) / train_std\ndf_std = df_std.melt(var_name='Column', value_name='Normalized')\nplt.figure(figsize=(12, 6))\nax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n_ = ax.set_xticklabels(mergeData.keys(), rotation=90)\nplt.savefig('violin_plot.png', format='png')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:57:38.131788Z","iopub.execute_input":"2023-10-02T19:57:38.132232Z","iopub.status.idle":"2023-10-02T19:57:38.557049Z","shell.execute_reply.started":"2023-10-02T19:57:38.132194Z","shell.execute_reply":"2023-10-02T19:57:38.55587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WindowGenerator():\n    # https://www.tensorflow.org/tutorials/structured_data/time_series#1_indexes_and_offsets\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_data, val_df=val_data, test_df=test_data,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:57:50.171894Z","iopub.execute_input":"2023-10-02T19:57:50.172349Z","iopub.status.idle":"2023-10-02T19:57:50.182343Z","shell.execute_reply.started":"2023-10-02T19:57:50.172306Z","shell.execute_reply":"2023-10-02T19:57:50.180948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_window(self, features):\n    # https://www.tensorflow.org/tutorials/structured_data/time_series#2_split\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nWindowGenerator.split_window = split_window\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:57:50.193133Z","iopub.execute_input":"2023-10-02T19:57:50.193557Z","iopub.status.idle":"2023-10-02T19:57:50.204787Z","shell.execute_reply.started":"2023-10-02T19:57:50.193521Z","shell.execute_reply":"2023-10-02T19:57:50.203453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot(self, model=None, plot_col='AggregateLoad', max_subplots=3):\n    # https://www.tensorflow.org/tutorials/structured_data/time_series#3_plot\n  inputs, labels = self.example\n  plt.figure(figsize=(12, 8))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    if model is not None:\n      predictions = model(inputs)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', edgecolors='k', label='Predictions',\n                  c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [h]')\n\nWindowGenerator.plot = plot","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:57:50.223118Z","iopub.execute_input":"2023-10-02T19:57:50.223512Z","iopub.status.idle":"2023-10-02T19:57:50.232351Z","shell.execute_reply.started":"2023-10-02T19:57:50.223479Z","shell.execute_reply":"2023-10-02T19:57:50.23144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_dataset(self, data):\n    # https://www.tensorflow.org/tutorials/structured_data/time_series#4_create_tfdatadatasets\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=True,\n      seed=randomState,\n      batch_size=32,)\n\n  ds = ds.map(self.split_window)\n\n  return ds\n\nWindowGenerator.make_dataset = make_dataset","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:57:50.252845Z","iopub.execute_input":"2023-10-02T19:57:50.253911Z","iopub.status.idle":"2023-10-02T19:57:50.26128Z","shell.execute_reply.started":"2023-10-02T19:57:50.253859Z","shell.execute_reply":"2023-10-02T19:57:50.260106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.tensorflow.org/tutorials/structured_data/time_series#4_create_tfdatadatasets\n@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:57:50.349239Z","iopub.execute_input":"2023-10-02T19:57:50.349636Z","iopub.status.idle":"2023-10-02T19:57:50.358971Z","shell.execute_reply.started":"2023-10-02T19:57:50.349606Z","shell.execute_reply":"2023-10-02T19:57:50.358141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare data for one-shot multi-step\nOUT_STEPS = 48 # 24 hour forecast\nIN_STEPS = 96 # look back 12 hours\nmulti_window = WindowGenerator(input_width=IN_STEPS,\n                               label_width=OUT_STEPS,\n                               shift=OUT_STEPS)\n\nmulti_window.plot()\nmulti_window","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:51:08.359545Z","iopub.execute_input":"2023-10-03T00:51:08.360084Z","iopub.status.idle":"2023-10-03T00:51:09.232572Z","shell.execute_reply.started":"2023-10-03T00:51:08.360042Z","shell.execute_reply":"2023-10-03T00:51:09.23127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use a naive persistence model as baseline to compare more sophisticated models\nUse a 1 week persistence\n\nGeorgios Tziolis, Chrysovalantis Spanias, Maria Theodoride, Spyros Theocharides, Javier Lopez-Lorente, Andreas Livera, George Makrides, George E. Georghiou,\n\nShort-term electric net load forecasting for solar-integrated distribution systems based on Bayesian neural networks and statistical post-processing,\n\nEnergy,\nVolume 271,\n2023,\n127018,\nISSN 0360-5442,\n\nhttps://doi.org/10.1016/j.energy.2023.127018.","metadata":{}},{"cell_type":"code","source":"# capture performnce of models\nmulti_val_performance = {}\nmulti_performance = {}","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:51:09.234712Z","iopub.execute_input":"2023-10-03T00:51:09.235872Z","iopub.status.idle":"2023-10-03T00:51:09.240562Z","shell.execute_reply.started":"2023-10-03T00:51:09.235835Z","shell.execute_reply":"2023-10-03T00:51:09.23947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.tensorflow.org/tutorials/structured_data/time_series#linear_model\nMAX_EPOCHS = 50\n\ndef compile_and_fit(model, window, patience=5):\n  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\n  checkpoint_filepath = '/tmp/' + model.name + '/checkpoint'\n  checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_filepath, \n                    monitor=\"val_loss\", mode=\"min\", \n                    save_best_only=True, verbose=1)\n    \n  history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping, checkpoint])\n\n  model.load_weights(checkpoint_filepath)\n    \n  return history\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:51:09.242085Z","iopub.execute_input":"2023-10-03T00:51:09.242411Z","iopub.status.idle":"2023-10-03T00:51:09.255361Z","shell.execute_reply.started":"2023-10-03T00:51:09.242384Z","shell.execute_reply":"2023-10-03T00:51:09.254297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Naive 1 week persistence model\nNaiveForecast = mergeData.AggregateLoad.copy()\n\nOneWeekNPeriods = 48 * 7\n\nNaiveForecast[:OneWeekNPeriods] = np.nan\n\nfor i in range(OneWeekNPeriods, len(mergeData.AggregateLoad)):\n    NaiveForecast[i] = mergeData.AggregateLoad[i - OneWeekNPeriods]\n\nprint(NaiveForecast.info())\nprint(NaiveForecast.describe())","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:51:09.257646Z","iopub.execute_input":"2023-10-03T00:51:09.257967Z","iopub.status.idle":"2023-10-03T00:51:09.67995Z","shell.execute_reply.started":"2023-10-03T00:51:09.257941Z","shell.execute_reply":"2023-10-03T00:51:09.678782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize naive forecast\nprediction_plot(mergeData.AggregateLoad, NaiveForecast)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:51:09.681736Z","iopub.execute_input":"2023-10-03T00:51:09.682571Z","iopub.status.idle":"2023-10-03T00:51:10.256014Z","shell.execute_reply.started":"2023-10-03T00:51:09.682525Z","shell.execute_reply":"2023-10-03T00:51:10.254702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize naive forecast and actuals for first day of the test dataset\nprint(train_size, val_size, test_size)\nprint(train_size+val_size)\nprediction_plot(mergeData.AggregateLoad[train_size+val_size:train_size+val_size+OUT_STEPS], NaiveForecast[train_size+val_size:train_size+val_size+OUT_STEPS])","metadata":{"execution":{"iopub.status.busy":"2023-10-03T13:40:26.443218Z","iopub.execute_input":"2023-10-03T13:40:26.444204Z","iopub.status.idle":"2023-10-03T13:40:26.497153Z","shell.execute_reply.started":"2023-10-03T13:40:26.444156Z","shell.execute_reply":"2023-10-03T13:40:26.495303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The average offer price for balancing actions  to align supply and demand  between the start of September and early January was 287 a MWh. Data from Elexon, which oversees the market, shows Rye House submitted the 20 highest winter bids  between 5,000 and 6,000 a MWh  for varying volumes of power on 12 December, setting new records\nhttps://www.theguardian.com/business/2023/jan/29/gas-fired-plants-uk-lights-on-cost-profits-energy-crisis","metadata":{}},{"cell_type":"code","source":"# Area Under Curve\nfrom scipy.integrate import simpson\nfrom numpy import trapz\n\n\n# The y values.  A numpy array is used here,\n# but a python list could also be used.\n# y = np.array([5, 20, 4, 18, 19, 18, 7, 4])\nyhat = NaiveForecast[train_size+test_size:train_size+test_size+OUT_STEPS]\n\n# Compute the area using the composite trapezoidal rule.\nyhatArea = trapz(yhat, dx=5)\nprint(\"yhat area =\", yhatArea)\n\n# Compute the area using the composite Simpson's rule.\nyhatArea = simpson(yhat, dx=5)\nprint(\"yhat area =\", yhatArea)\n\nyActual = mergeData.AggregateLoad[train_size+test_size:train_size+test_size+OUT_STEPS]\n\n# Compute the area using the composite trapezoidal rule.\nyActualArea = trapz(yActual, dx=5)\nprint(\"yActualArea =\", yActualArea)\n\n# Compute the area using the composite Simpson's rule.\nyActualArea = simpson(yActual, dx=5)\nprint(\"yActualArea =\", yActualArea)\n\n# Calculate the area difference between the two curves...\nAreaDifference = yhatArea - yActualArea\nprint(\"AreaDifference =\", AreaDifference)\n\n# calculate price at 287 a MWh\ncost = (AreaDifference / 1000) * 287\nprint(\"cost for 1 day for 5k homes =\", cost)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:51:10.5672Z","iopub.execute_input":"2023-10-03T00:51:10.567609Z","iopub.status.idle":"2023-10-03T00:51:10.920565Z","shell.execute_reply.started":"2023-10-03T00:51:10.567575Z","shell.execute_reply":"2023-10-03T00:51:10.916505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The population of England was 56,489,800 in 2021\n24.9 million dwellings in England\n\n","metadata":{}},{"cell_type":"code","source":"# Scale up costs\ncost = cost * (56000000/5000)\nprint(\"cost for 1 day for 5k homes =\", cost)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:51:10.921429Z","iopub.status.idle":"2023-10-03T00:51:10.921818Z","shell.execute_reply.started":"2023-10-03T00:51:10.921631Z","shell.execute_reply":"2023-10-03T00:51:10.921648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# normalize the naive \n# (NaiveForecast - train_mean) / train_std\nNaiveForecastNormed = NaiveForecast.transform(lambda x: (x - train_mean) / train_std)\nNaiveForecastNormed.describe()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:51:10.926765Z","iopub.status.idle":"2023-10-03T00:51:10.927395Z","shell.execute_reply.started":"2023-10-03T00:51:10.927092Z","shell.execute_reply":"2023-10-03T00:51:10.92712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize naive forecast and actuals for first day of the test dataset\nmergeDataNormed = (mergeData - train_mean) / train_std\nprediction_plot(mergeDataNormed.AggregateLoad[train_size+test_size:train_size+test_size+OUT_STEPS], NaiveForecastNormed.AggregateLoad[train_size+test_size:train_size+test_size+OUT_STEPS])","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:51:10.928586Z","iopub.status.idle":"2023-10-03T00:51:10.928957Z","shell.execute_reply.started":"2023-10-03T00:51:10.92878Z","shell.execute_reply":"2023-10-03T00:51:10.928797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(mergeDataNormed.AggregateLoad[train_size:train_size+val_size])\nprint(NaiveForecastNormed.AggregateLoad[train_size:train_size+val_size])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:51:10.930487Z","iopub.status.idle":"2023-10-03T00:51:10.931049Z","shell.execute_reply.started":"2023-10-03T00:51:10.93084Z","shell.execute_reply":"2023-10-03T00:51:10.93086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate error for naive model (Normed)\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n\n# calculate error for naive model on validation set\nvalNaiveMAE = mean_absolute_error(mergeDataNormed.AggregateLoad[train_size:train_size+val_size], NaiveForecastNormed.AggregateLoad[train_size:train_size+val_size])\n\n# calculate error for naive model on test set\ntestNaiveMAE = mean_absolute_error(mergeDataNormed.AggregateLoad[train_size+val_size:], NaiveForecastNormed.AggregateLoad[train_size+val_size:])\n\nprint('valNaiveMAE: ', valNaiveMAE)\nprint('testNaiveMAE: ', testNaiveMAE)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:51:10.93186Z","iopub.status.idle":"2023-10-03T00:51:10.932232Z","shell.execute_reply.started":"2023-10-03T00:51:10.932058Z","shell.execute_reply":"2023-10-03T00:51:10.932076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate error for naive model (not normed)\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\nprint('Naive Root Mean Squared Error(RMSE): %.2f; Naive Mean Absolute Error(MAE) : %.2f; Naive Mean Absolute Percantage Error(MAPE) : %.2f '\n      % (np.sqrt(mean_squared_error(mergeData.AggregateLoad[OneWeekNPeriods:], NaiveForecast[OneWeekNPeriods:])),\n         mean_absolute_error(mergeData.AggregateLoad[OneWeekNPeriods:], NaiveForecast[OneWeekNPeriods:]),\n         mean_absolute_percentage_error(mergeData.AggregateLoad[OneWeekNPeriods:], NaiveForecast[OneWeekNPeriods:])))\n\n# calculate error for naive model on validation set\nvalNaiveMAE = mean_absolute_error(mergeData.AggregateLoad[train_size:train_size+val_size], NaiveForecast[train_size:train_size+val_size])\n\n# calculate error for naive model on test set\ntestNaiveMAE = mean_absolute_error(mergeData.AggregateLoad[train_size+val_size:], NaiveForecast[train_size+val_size:])\n\nprint('valNaiveMAE: ', valNaiveMAE)\nprint('testNaiveMAE: ', testNaiveMAE)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:51:10.933783Z","iopub.status.idle":"2023-10-03T00:51:10.934155Z","shell.execute_reply.started":"2023-10-03T00:51:10.933968Z","shell.execute_reply":"2023-10-03T00:51:10.934005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for plotting the train and test loss curves\ndef plot_model_loss(history):\n    plt.figure(figsize=(8,4))\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epochs')\n    plt.legend(loc='upper right')\n    plt.show()\n    return","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:52:04.575941Z","iopub.execute_input":"2023-10-03T00:52:04.576778Z","iopub.status.idle":"2023-10-03T00:52:04.583757Z","shell.execute_reply.started":"2023-10-03T00:52:04.576738Z","shell.execute_reply":"2023-10-03T00:52:04.582501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RepeatBaseline(tf.keras.Model):\n  def call(self, inputs):\n    return inputs\n\nrepeat_baseline = RepeatBaseline()\nrepeat_baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n                        metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\nmulti_val_performance['Naive'] = repeat_baseline.evaluate(mergeDataNormed.AggregateLoad[train_size:train_size+val_size], NaiveForecastNormed.AggregateLoad[train_size:train_size+val_size], verbose=0)\nmulti_performance['Naive'] = repeat_baseline.evaluate(mergeDataNormed.AggregateLoad[train_size+val_size:], NaiveForecastNormed.AggregateLoad[train_size+val_size:], verbose=0)\n\n# multi_val_performance['Repeat'] = repeat_baseline.evaluate(multi_window.val)\n# multi_performance['Repeat'] = repeat_baseline.evaluate(multi_window.test, verbose=0)\n# multi_window.plot(repeat_baseline)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:33:05.660741Z","iopub.execute_input":"2023-10-03T01:33:05.661196Z","iopub.status.idle":"2023-10-03T01:33:06.144877Z","shell.execute_reply.started":"2023-10-03T01:33:05.661164Z","shell.execute_reply":"2023-10-03T01:33:06.143694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_dense_model = tf.keras.Sequential([\n    # Take the last time step.\n    # Shape [batch, time, features] => [batch, 1, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n    # Shape => [batch, 1, dense_units]\n    tf.keras.layers.Dense(256, activation='relu'),\n    # Shape => [batch, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_dense_model, multi_window)\nmulti_dense_model.save('multi_dense_model.keras')\nplot_model_loss(history)\n\n# IPython.display.clear_output()\nmulti_val_performance['Dense'] = multi_dense_model.evaluate(multi_window.val)\nmulti_performance['Dense'] = multi_dense_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_dense_model)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:52:04.771332Z","iopub.execute_input":"2023-10-03T00:52:04.772093Z","iopub.status.idle":"2023-10-03T00:52:22.39325Z","shell.execute_reply.started":"2023-10-03T00:52:04.772053Z","shell.execute_reply":"2023-10-03T00:52:22.392093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN","metadata":{}},{"cell_type":"code","source":"CONV_WIDTH = 10\nmulti_conv_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n    # Shape => [batch, 1, conv_units]\n    tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),\n    # Shape => [batch, 1,  out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_conv_model, multi_window)\nmulti_conv_model.save('multi_conv_model.keras')\nplot_model_loss(history)\n# IPython.display.clear_output()\n\nmulti_val_performance['Conv'] = multi_conv_model.evaluate(multi_window.val)\nmulti_performance['Conv'] = multi_conv_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_conv_model)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:52:22.394801Z","iopub.execute_input":"2023-10-03T00:52:22.395214Z","iopub.status.idle":"2023-10-03T00:53:20.826402Z","shell.execute_reply.started":"2023-10-03T00:52:22.395179Z","shell.execute_reply":"2023-10-03T00:53:20.825081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_rnn_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(256, return_sequences=False),\n    # Shape => [batch, out_steps*features].\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features].\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_rnn_model, multi_window)\nplot_model_loss(history)\nmulti_rnn_model.save('multi_rnn_model.keras')\n# IPython.display.clear_output()\n\nmulti_val_performance['RNN'] = multi_rnn_model.evaluate(multi_window.val)\nmulti_performance['RNN'] = multi_rnn_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_rnn_model)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:53:20.829197Z","iopub.execute_input":"2023-10-03T00:53:20.829682Z","iopub.status.idle":"2023-10-03T00:57:28.709103Z","shell.execute_reply.started":"2023-10-03T00:53:20.829645Z","shell.execute_reply":"2023-10-03T00:57:28.707858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM Model","metadata":{}},{"cell_type":"code","source":"multi_lstm_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units].\n    # Adding more `lstm_units` just overfits more quickly.\n    tf.keras.layers.LSTM(256, return_sequences=False),\n    # Shape => [batch, out_steps*features].\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features].\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nstart_time = time.time()\nhistory = compile_and_fit(multi_lstm_model, multi_window)\nprint('%s seconds' % (time.time() - start_time))\nmulti_lstm_model.save('multi_lstm_model.keras')\nplot_model_loss(history)\n# IPython.display.clear_output()\n\nmulti_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val)\nmulti_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_lstm_model)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T00:57:28.710849Z","iopub.execute_input":"2023-10-03T00:57:28.712071Z","iopub.status.idle":"2023-10-03T01:25:03.729329Z","shell.execute_reply.started":"2023-10-03T00:57:28.71202Z","shell.execute_reply":"2023-10-03T01:25:03.728056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN-LSTM","metadata":{}},{"cell_type":"code","source":"print(multi_val_performance)\nprint(multi_performance)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:03.731112Z","iopub.execute_input":"2023-10-03T01:25:03.731486Z","iopub.status.idle":"2023-10-03T01:25:03.737648Z","shell.execute_reply.started":"2023-10-03T01:25:03.731451Z","shell.execute_reply":"2023-10-03T01:25:03.736289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.arange(len(multi_performance))\nwidth = 0.3\n\nmetric_name = 'mean_absolute_error'\nmetric_index = multi_lstm_model.metrics_names.index('mean_absolute_error')\nval_mae = [v[metric_index] for v in multi_val_performance.values()]\ntest_mae = [v[metric_index] for v in multi_performance.values()]\n\nplt.bar(x - 0.17, val_mae, width, label='Validation')\nplt.bar(x + 0.17, test_mae, width, label='Test')\nplt.xticks(ticks=x, labels=multi_performance.keys(),\n           rotation=45)\nplt.ylabel(f'MAE (average over all times and outputs)')\n_ = plt.legend()\nplt.savefig('model performance.png', format='png')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:33:33.950402Z","iopub.execute_input":"2023-10-03T01:33:33.950816Z","iopub.status.idle":"2023-10-03T01:33:34.282384Z","shell.execute_reply.started":"2023-10-03T01:33:33.950785Z","shell.execute_reply":"2023-10-03T01:33:34.281194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build a single record dataset from start of the test dataset\nprint(mergeDataNormed)\nprint(train_size+test_size-IN_STEPS)\nprint(mergeDataNormed.iloc[train_size+test_size-IN_STEPS:train_size+test_size])\n\nds = tf.keras.utils.timeseries_dataset_from_array(\n      data=test_data[:IN_STEPS],\n      targets=None,\n      sequence_length=IN_STEPS,\n      sequence_stride=1,\n      shuffle=False,\n      batch_size=32,)\n\ntestYhatNormed = multi_lstm_model.predict(ds)\n\n#     data=test_data[:IN_STEPS],\n# data=mergeDataNormed[train_size+test_size-IN_STEPS:train_size+test_size],","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.083173Z","iopub.status.idle":"2023-10-03T01:25:04.083586Z","shell.execute_reply.started":"2023-10-03T01:25:04.083405Z","shell.execute_reply":"2023-10-03T01:25:04.083423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(testYhatNormed)\nprint(testYhatNormed[0,:,-1])","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.084904Z","iopub.status.idle":"2023-10-03T01:25:04.085308Z","shell.execute_reply.started":"2023-10-03T01:25:04.085131Z","shell.execute_reply":"2023-10-03T01:25:04.08515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Invert standardization\ntestYhat = (np.array(testYhatNormed) * np.array(train_std)) + np.array(train_mean)\n\nprint(testYhat)\nprint(testYhat[0,:,-1])","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.087138Z","iopub.status.idle":"2023-10-03T01:25:04.087497Z","shell.execute_reply.started":"2023-10-03T01:25:04.087324Z","shell.execute_reply":"2023-10-03T01:25:04.087348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CNN-LSTM","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.088458Z","iopub.status.idle":"2023-10-03T01:25:04.089068Z","shell.execute_reply.started":"2023-10-03T01:25:04.088839Z","shell.execute_reply":"2023-10-03T01:25:04.088859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot of naive, LSTM and actuals","metadata":{}},{"cell_type":"code","source":"      len_prediction=[x for x in range(len(testYhat[0,:,-1]))]\n      plt.figure(figsize=(10,5))\n      # plt.plot(len_prediction, test_data[:OUT_STEPS].AggregateLoad, marker='.', label=\"actual\")\n      plt.plot(len_prediction, mergeDataNormed.AggregateLoad[train_size+val_size:train_size+val_size+OUT_STEPS], marker='.', label=\"actual\")\n      plt.plot(len_prediction, testYhatNormed[0,:,-1], 'r', label=\"LSTM prediction\")\n      plt.plot(len_prediction, NaiveForecastNormed[train_size+val_size:train_size+val_size+OUT_STEPS].AggregateLoad, 'g', label=\"Naive prediction\")\n        \n      plt.tight_layout()\n      sns.despine(top=True)\n      plt.subplots_adjust(left=0.07)\n      plt.ylabel('KWH per half hour (Normed)', size=15)\n      plt.xlabel('Time step', size=15)\n      plt.legend(fontsize=15)\n      plt.show();\n      plt.savefig('Test First Day Normed.png', format='png')\n\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.09031Z","iopub.status.idle":"2023-10-03T01:25:04.090902Z","shell.execute_reply.started":"2023-10-03T01:25:04.09071Z","shell.execute_reply":"2023-10-03T01:25:04.090729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"      len_prediction=[x for x in range(len(testYhat[0,:,-1]))]\n      plt.figure(figsize=(10,5))\n      # plt.plot(len_prediction, test_data[:OUT_STEPS].AggregateLoad, marker='.', label=\"actual\")\n      plt.plot(len_prediction, mergeData.AggregateLoad[train_size+val_size:train_size+val_size+OUT_STEPS], marker='.', label=\"actual\")\n      plt.plot(len_prediction, testYhat[0,:,-1], 'r', label=\"LSTM prediction\")\n      plt.plot(len_prediction, NaiveForecast[train_size+val_size:train_size+val_size+OUT_STEPS], 'g', label=\"Naive prediction\")\n        \n      plt.tight_layout()\n      sns.despine(top=True)\n      plt.subplots_adjust(left=0.07)\n      plt.ylabel('KWH per half hour', size=15)\n      plt.xlabel('Time step', size=15)\n      plt.legend(fontsize=15)\n      plt.show()\n      plt.savefig('Test First Day.png', format='png')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.092227Z","iopub.status.idle":"2023-10-03T01:25:04.092706Z","shell.execute_reply.started":"2023-10-03T01:25:04.092466Z","shell.execute_reply":"2023-10-03T01:25:04.092489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Estimate financial impact of improved forecast","metadata":{}},{"cell_type":"code","source":"# Area Under Curve\nfrom scipy.integrate import simpson\nfrom numpy import trapz\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.095171Z","iopub.status.idle":"2023-10-03T01:25:04.095524Z","shell.execute_reply.started":"2023-10-03T01:25:04.09536Z","shell.execute_reply":"2023-10-03T01:25:04.095376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# install AutoGluon AutoML\n!pip install autogluon\nfrom autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.096221Z","iopub.status.idle":"2023-10-03T01:25:04.096556Z","shell.execute_reply.started":"2023-10-03T01:25:04.096396Z","shell.execute_reply":"2023-10-03T01:25:04.096411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoData = pd.read_csv('/kaggle/working/autoDataFinal.csv')\nautoData","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.097807Z","iopub.status.idle":"2023-10-03T01:25:04.098208Z","shell.execute_reply.started":"2023-10-03T01:25:04.098027Z","shell.execute_reply":"2023-10-03T01:25:04.098048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AutoGluon specific data preparation\n# Split the time series data into train, test, and validation datasets\ntrain_size = int(len(autoData) * 0.7)  # 70% for training\nval_size = int(len(autoData) * 0.2)   # 20% for validation\ntest_size = len(autoData) - val_size - test_size  # Remaining for testing\n\nag_train_data = autoData[:train_size]\nag_val_data = autoData[train_size:train_size+val_size]\nag_test_data = autoData[train_size+val_size:]\n\nprint('ag_train_data:\\n', ag_train_data)\nprint('ag_val_data:\\n', ag_val_data)\nprint('ag_test_data\\n', ag_test_data)\n# print(ag_train_data.info())\n\n# AutoGluon requires an ItemID Column, so adding one...\nag_train_data['item_id'] = 'LoadSum'\nag_train_data = ag_train_data.astype({\"item_id\": str})\nag_val_data['item_id'] = 'LoadSum'\nag_val_data = ag_val_data.astype({\"item_id\": str})\nag_test_data['item_id'] = 'LoadSum'\nag_test_data = ag_test_data.astype({\"item_id\": str})\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.099466Z","iopub.status.idle":"2023-10-03T01:25:04.099824Z","shell.execute_reply.started":"2023-10-03T01:25:04.099653Z","shell.execute_reply":"2023-10-03T01:25:04.09967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a quick look at the split datasets\nprint('ag_train_data\\n', ag_train_data)\nprint('ag_val_data\\n', ag_val_data)\nprint('ag_test_data\\n', ag_test_data)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.10134Z","iopub.status.idle":"2023-10-03T01:25:04.101926Z","shell.execute_reply.started":"2023-10-03T01:25:04.101532Z","shell.execute_reply":"2023-10-03T01:25:04.10155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load training data in to required AutoGluon proprietary data frame\n# print(ag_train_data.info())\nag_train_data_tsdf = TimeSeriesDataFrame.from_data_frame(\n    ag_train_data,\n    id_column=\"item_id\",\n    timestamp_column=\"DateTime\"\n)\nag_train_data_tsdf","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.10325Z","iopub.status.idle":"2023-10-03T01:25:04.103608Z","shell.execute_reply.started":"2023-10-03T01:25:04.103437Z","shell.execute_reply":"2023-10-03T01:25:04.103456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load validation data in to required AutoGluon proprietary data frame, \"_tsdf\" suffix = time series data frame\nag_val_data_tsdf = TimeSeriesDataFrame.from_data_frame(\n    ag_val_data,\n    id_column=\"item_id\",\n    timestamp_column=\"DateTime\"\n)\nag_val_data_tsdf","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.10507Z","iopub.status.idle":"2023-10-03T01:25:04.105435Z","shell.execute_reply.started":"2023-10-03T01:25:04.10526Z","shell.execute_reply":"2023-10-03T01:25:04.105277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load test data in to required AutoGluon proprietary data frame\n# print(ag_test_data.info())\nag_test_data_tsdf = TimeSeriesDataFrame.from_data_frame(\n    ag_test_data,\n    id_column=\"item_id\",\n    timestamp_column=\"DateTime\"\n)\nag_test_data_tsdf","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.106759Z","iopub.status.idle":"2023-10-03T01:25:04.107171Z","shell.execute_reply.started":"2023-10-03T01:25:04.106953Z","shell.execute_reply":"2023-10-03T01:25:04.106971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# at \"high_quality\" level, training takes about 45 minutes...\n# training takes about 15 minutes for DeepAR\n# training takes about 21 minutes for TemporalFusionTransformer\n# training takes about 4 minutes for PatchTST\n# training takes about 4 minutes for PatchTST","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.108299Z","iopub.status.idle":"2023-10-03T01:25:04.108657Z","shell.execute_reply.started":"2023-10-03T01:25:04.108485Z","shell.execute_reply":"2023-10-03T01:25:04.108503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ag_predictor = TimeSeriesPredictor(\n    prediction_length=48,\n    path=\"autogluon-london-half-hourly\",\n    target=\"AggregateLoad\",\n    eval_metric=\"MASE\",\n)\n\nag_predictor.fit(\n    ag_train_data_tsdf,\n    presets=\"medium_quality\",\n    time_limit=6000,\n    random_seed=randomState\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.110379Z","iopub.status.idle":"2023-10-03T01:25:04.111149Z","shell.execute_reply.started":"2023-10-03T01:25:04.110904Z","shell.execute_reply":"2023-10-03T01:25:04.110926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The test score is computed using the last\n# prediction_length=48 timesteps of each time series in test_data\nag_predictor.leaderboard(ag_val_data_tsdf, silent=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.112469Z","iopub.status.idle":"2023-10-03T01:25:04.113139Z","shell.execute_reply.started":"2023-10-03T01:25:04.112906Z","shell.execute_reply":"2023-10-03T01:25:04.112928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate predictions\nag_val_predictions = ag_predictor.predict(ag_val_data_tsdf)\nag_val_predictions","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.114611Z","iopub.status.idle":"2023-10-03T01:25:04.115054Z","shell.execute_reply.started":"2023-10-03T01:25:04.114841Z","shell.execute_reply":"2023-10-03T01:25:04.114861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_mean)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.116252Z","iopub.status.idle":"2023-10-03T01:25:04.118836Z","shell.execute_reply.started":"2023-10-03T01:25:04.118632Z","shell.execute_reply":"2023-10-03T01:25:04.118655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# standardize the predictions so we can compare prediction errors with other models\nag_val_predictionsNormed = (ag_val_predictions[\"mean\"] - train_mean[\"AggregateLoad\"]) / train_std[\"AggregateLoad\"]\nag_val_predictionsNormed","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.120107Z","iopub.status.idle":"2023-10-03T01:25:04.120491Z","shell.execute_reply.started":"2023-10-03T01:25:04.120307Z","shell.execute_reply":"2023-10-03T01:25:04.120326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot prediction results, history and actual test data values\nplt.figure(figsize=(20, 3))\n\nitem_id = \"LoadSum\"\ny_past = ag_val_data_tsdf.loc[item_id][\"AggregateLoad\"]\ny_pred = ag_predictions.loc[item_id]\ny_val = ag_test_data_tsdf.loc[item_id][\"AggregateLoad\"]\n# y_val = ag_val_data_tsdf.loc[item_id][\"AggregateLoad\"]\n\nplt.plot(y_past[-100:], label=\"Past time series values\")\nplt.plot(y_pred[\"mean\"], label=\"Mean forecast\")\nplt.plot(y_val[:48], label=\"Future time series values\")\n\nplt.fill_between(\n    y_pred.index, y_pred[\"0.1\"], y_pred[\"0.9\"], color=\"red\", alpha=0.1, label=f\"10%-90% confidence interval\"\n)\nplt.legend();\nplt.savefig('AutoML forecast.png', format='png')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.121843Z","iopub.status.idle":"2023-10-03T01:25:04.122593Z","shell.execute_reply.started":"2023-10-03T01:25:04.122369Z","shell.execute_reply":"2023-10-03T01:25:04.122396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# load test data in to required AutoGluon proprietary data frame\n# print(ag_test_data.info())\nag_testday1_data_tsdf = TimeSeriesDataFrame.from_data_frame(\n    ag_val_data,\n    id_column=\"item_id\",\n    timestamp_column=\"DateTime\"\n)\nag_testday1_data_tsdf","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.124244Z","iopub.status.idle":"2023-10-03T01:25:04.124637Z","shell.execute_reply.started":"2023-10-03T01:25:04.124454Z","shell.execute_reply":"2023-10-03T01:25:04.124474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate predictions\nag_test_predictions = ag_predictor.predict(ag_test_data_tsdf)\nag_test_predictions.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.126156Z","iopub.status.idle":"2023-10-03T01:25:04.126529Z","shell.execute_reply.started":"2023-10-03T01:25:04.126355Z","shell.execute_reply":"2023-10-03T01:25:04.126374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ag_test_predictions","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.128099Z","iopub.status.idle":"2023-10-03T01:25:04.128442Z","shell.execute_reply.started":"2023-10-03T01:25:04.128275Z","shell.execute_reply":"2023-10-03T01:25:04.128291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# standardize the predictions so we can compare prediction errors with other models\nag_test_predictionsNormed = (ag_test_predictions[\"mean\"] - train_mean[\"AggregateLoad\"]) / train_std[\"AggregateLoad\"]\nag_test_predictionsNormed","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.129095Z","iopub.status.idle":"2023-10-03T01:25:04.129464Z","shell.execute_reply.started":"2023-10-03T01:25:04.129274Z","shell.execute_reply":"2023-10-03T01:25:04.129291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"      len_prediction=[x for x in range(len(testYhat[0,:,-1]))]\n      plt.figure(figsize=(10,5))\n      # plt.plot(len_prediction, test_data[:OUT_STEPS].AggregateLoad, marker='.', label=\"actual\")\n      plt.plot(len_prediction, mergeData.AggregateLoad[train_size+val_size:train_size+val_size+OUT_STEPS], marker='.', label=\"actual\")\n      plt.plot(len_prediction, testYhat[0,:,-1], 'r', label=\"CNN prediction\")\n      plt.plot(len_prediction, NaiveForecast[train_size+val_size:train_size+val_size+OUT_STEPS], 'g', label=\"Naive prediction\")\n      plt.plot(len_prediction, ag_val_predictions[:OUT_STEPS][\"mean\"], 'y', label=\"AutoML prediction\")\n        \n      plt.tight_layout()\n      sns.despine(top=True)\n      plt.subplots_adjust(left=0.07)\n      plt.ylabel('KWH per half hour', size=15)\n      plt.xlabel('Time step', size=15)\n      plt.legend(fontsize=15)\n      plt.show()\n      plt.savefig('Test First Day.png', format='png')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.130635Z","iopub.status.idle":"2023-10-03T01:25:04.131014Z","shell.execute_reply.started":"2023-10-03T01:25:04.130818Z","shell.execute_reply":"2023-10-03T01:25:04.130836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nmetric = ag_predictor.evaluate(ag_test_data_tsdf, metric='MSE')\nprint('Test MSE: ', metric)\nrootMSE = math.sqrt(abs(metric))\nprint('Test root MSE: ', rootMSE)\n# norm the error\nnormedRootMSE = (rootMSE - train_mean[\"AggregateLoad\"]) / train_std[\"AggregateLoad\"]\nprint('Normed Test root MSE: ', normedRootMSE)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.132176Z","iopub.status.idle":"2023-10-03T01:25:04.132545Z","shell.execute_reply.started":"2023-10-03T01:25:04.132362Z","shell.execute_reply":"2023-10-03T01:25:04.132379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metric\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.133779Z","iopub.status.idle":"2023-10-03T01:25:04.134199Z","shell.execute_reply.started":"2023-10-03T01:25:04.134006Z","shell.execute_reply":"2023-10-03T01:25:04.134031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate error for naive model on validation set\n# valAutoMLMAE = mean_absolute_error(mergeDataNormed.AggregateLoad[train_size:train_size+val_size], ag_predictionsNormed[train_size:train_size+val_size])\n\nprint(mergeDataNormed.AggregateLoad[train_size+val_size:train_size+val_size+OUT_STEPS])\nprint(ag_test_predictionsNormed)\n# calculate error for naive model on test set\ntestAutoMLMAE = mean_absolute_error(mergeDataNormed.AggregateLoad[train_size+val_size:train_size+val_size+OUT_STEPS], ag_test_predictionsNormed)\n\n# print('valAutoMLMAE: ', valAutoMLMAE)\nprint('testAutoMLMAE: ', testAutoMLMAE)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.135276Z","iopub.status.idle":"2023-10-03T01:25:04.135626Z","shell.execute_reply.started":"2023-10-03T01:25:04.135461Z","shell.execute_reply":"2023-10-03T01:25:04.135477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import dill\n# save\ndill.dump_session('notebook_env.db')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T01:25:04.137158Z","iopub.status.idle":"2023-10-03T01:25:04.137512Z","shell.execute_reply.started":"2023-10-03T01:25:04.137347Z","shell.execute_reply":"2023-10-03T01:25:04.137363Z"},"trusted":true},"execution_count":null,"outputs":[]}]}