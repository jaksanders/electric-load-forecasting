{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Short-term residential load forecasting with Deep Learning\n\nLondon Households SmartMeter Data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-01T23:52:33.392443Z","iopub.execute_input":"2023-10-01T23:52:33.393135Z","iopub.status.idle":"2023-10-01T23:52:33.486835Z","shell.execute_reply.started":"2023-10-01T23:52:33.393081Z","shell.execute_reply":"2023-10-01T23:52:33.485382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nimport IPython\nimport IPython.display\nimport glob\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nrandomState = 42 # tip of the cap to Douglas Adams\n#!pip install -U pip\n#!pip install -U setuptools wheel\n\n#!pip install autogluon\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:52:33.488789Z","iopub.execute_input":"2023-10-01T23:52:33.489113Z","iopub.status.idle":"2023-10-01T23:52:33.496850Z","shell.execute_reply.started":"2023-10-01T23:52:33.489082Z","shell.execute_reply":"2023-10-01T23:52:33.494944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creat two load forecasts...\n1) half-hourly load forecast for next 24 hours\n2) peak half-hour in the next 24 hours\n\nValue...\n* For electric network operator, minimize the amount of spinning reserve\n\nData...\n* residential smart meter usage data\n* weather data\n* weather forecast data","metadata":{}},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"markdown","source":"## Load half-hourly electric usage data\n...for ~5k smart meters in London\n[SmartMeter Energy Consumption Data in London Households](https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households)","metadata":{}},{"cell_type":"code","source":"import time\n# load half-hourly electric usage data\n# takes about four minutes, need to find somerthing faster like Dask?\n# https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households\nd = pd.read_csv('/kaggle/input/small-lcl-data/LCL-June2015v2_99.csv', parse_dates=[\"DateTime\"])\n\n# Get CSV files list from a folder\npath = '/kaggle/input/smart-meters-in-london/halfhourly_dataset/halfhourly_dataset'\ncsv_files = glob.glob(path + \"/*.csv\")\n\n# Read each CSV file into DataFrame\n# This creates a list of dataframes\nstart_time = time.time()\ndf_list = (pd.read_csv(file, parse_dates=[\"tstp\"]) for file in csv_files)\nprint('%s seconds' % (time.time() - start_time))\n\n# Concatenate all DataFrames\nstart_time = time.time()\nd = pd.concat(df_list, ignore_index=True)\nprint('%s seconds' % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:52:33.498827Z","iopub.execute_input":"2023-10-01T23:52:33.499185Z","iopub.status.idle":"2023-10-01T23:56:47.870525Z","shell.execute_reply.started":"2023-10-01T23:52:33.499159Z","shell.execute_reply":"2023-10-01T23:56:47.869422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:56:47.874431Z","iopub.execute_input":"2023-10-01T23:56:47.875399Z","iopub.status.idle":"2023-10-01T23:57:51.160257Z","shell.execute_reply.started":"2023-10-01T23:56:47.875347Z","shell.execute_reply":"2023-10-01T23:57:51.158848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load hourly weather data","metadata":{}},{"cell_type":"code","source":"# load hourly weather data\n# https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households\nweatherData = pd.read_csv('/kaggle/input/smart-meters-in-london/weather_hourly_darksky.csv', parse_dates=[\"time\"])","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:57:51.162009Z","iopub.execute_input":"2023-10-01T23:57:51.163556Z","iopub.status.idle":"2023-10-01T23:57:51.254859Z","shell.execute_reply.started":"2023-10-01T23:57:51.163513Z","shell.execute_reply":"2023-10-01T23:57:51.253289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weatherData.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:57:51.257653Z","iopub.execute_input":"2023-10-01T23:57:51.258610Z","iopub.status.idle":"2023-10-01T23:57:51.303190Z","shell.execute_reply.started":"2023-10-01T23:57:51.258546Z","shell.execute_reply":"2023-10-01T23:57:51.301677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weatherData.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:57:51.304982Z","iopub.execute_input":"2023-10-01T23:57:51.305346Z","iopub.status.idle":"2023-10-01T23:57:51.333986Z","shell.execute_reply.started":"2023-10-01T23:57:51.305318Z","shell.execute_reply":"2023-10-01T23:57:51.332626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data pre-processing and cleaning","metadata":{}},{"cell_type":"markdown","source":"## Weather data: convert text attributes to string datatype","metadata":{}},{"cell_type":"code","source":"weatherData = weatherData.astype({'precipType':'string', 'icon':'string', 'summary':'string'})\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:57:51.336267Z","iopub.execute_input":"2023-10-01T23:57:51.336780Z","iopub.status.idle":"2023-10-01T23:57:51.351417Z","shell.execute_reply.started":"2023-10-01T23:57:51.336745Z","shell.execute_reply":"2023-10-01T23:57:51.349880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas_profiling import ProfileReport\n\nprofile = ProfileReport(weatherData, tsmode=True, sortby=\"time\")\nprofile.to_file('weatherData profile_report.html')\n# profile\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:57:51.352974Z","iopub.execute_input":"2023-10-01T23:57:51.353422Z","iopub.status.idle":"2023-10-01T23:59:04.094791Z","shell.execute_reply.started":"2023-10-01T23:57:51.353370Z","shell.execute_reply":"2023-10-01T23:59:04.090775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Identify and remove weather records not exactly on the hour","metadata":{}},{"cell_type":"code","source":"# inspect and remove records not exactly on the hour\noffRecs = weatherData.query(\"time.dt.minute != 0 or time.dt.second != 0\")\nprint('Records not exactly on the half-hour:\\n ', offRecs)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:59:04.105362Z","iopub.execute_input":"2023-10-01T23:59:04.109678Z","iopub.status.idle":"2023-10-01T23:59:04.163486Z","shell.execute_reply.started":"2023-10-01T23:59:04.109568Z","shell.execute_reply":"2023-10-01T23:59:04.161001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Upsample weather data to match half-houly sampling rate of load data","metadata":{}},{"cell_type":"code","source":"# select weather data features of interest\nweatherUpsample = weatherData[['time','temperature', 'dewPoint', 'pressure', 'humidity']].copy()\nweatherUpsample = weatherUpsample.sort_values(by=['time'])\nprint(weatherUpsample.info())\nprint(weatherUpsample.describe())\nprint(weatherUpsample)\n\nweatherUpsample = weatherUpsample.set_index('time')\nweatherUpsample.index.rename('time', inplace=True)\n\nstart_time = time.time()\n\nweatherUpsample = weatherUpsample.resample('30Min').mean()\n\n# upsample \nweatherUpsample['temperature'] = weatherUpsample['temperature'].interpolate()\nweatherUpsample['dewPoint'] = weatherUpsample['dewPoint'].interpolate()\nweatherUpsample['pressure'] = weatherUpsample['pressure'].interpolate()\nweatherUpsample['humidity'] = weatherUpsample['humidity'].interpolate()\n\nprint('%s seconds' % (time.time() - start_time))\n\nweatherUpsample = weatherUpsample.reset_index(names='DateTime')\nprint(weatherUpsample.info())\nprint(weatherUpsample.describe())\nprint(weatherUpsample)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:59:04.166770Z","iopub.execute_input":"2023-10-01T23:59:04.167564Z","iopub.status.idle":"2023-10-01T23:59:04.423929Z","shell.execute_reply.started":"2023-10-01T23:59:04.167525Z","shell.execute_reply":"2023-10-01T23:59:04.419861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weatherUpsample.to_csv('/kaggle/working/WeatherDataFinal.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:59:04.428761Z","iopub.execute_input":"2023-10-01T23:59:04.430115Z","iopub.status.idle":"2023-10-01T23:59:04.943800Z","shell.execute_reply.started":"2023-10-01T23:59:04.429952Z","shell.execute_reply":"2023-10-01T23:59:04.942121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weatherUpsample = pd.read_csv('/kaggle/working/WeatherDataFinal.csv', parse_dates=[\"DateTime\"])\nweatherUpsample","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:59:04.946196Z","iopub.execute_input":"2023-10-01T23:59:04.946921Z","iopub.status.idle":"2023-10-01T23:59:05.032339Z","shell.execute_reply.started":"2023-10-01T23:59:04.946887Z","shell.execute_reply":"2023-10-01T23:59:05.030942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:59:05.033939Z","iopub.execute_input":"2023-10-01T23:59:05.034494Z","iopub.status.idle":"2023-10-01T23:59:05.047582Z","shell.execute_reply.started":"2023-10-01T23:59:05.034440Z","shell.execute_reply":"2023-10-01T23:59:05.046664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# utility function to nicely format variable names and memory they are consuming\nimport sys\ndef sizeof_fmt(num, suffix='B'):\n    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:59:05.048866Z","iopub.execute_input":"2023-10-01T23:59:05.049319Z","iopub.status.idle":"2023-10-01T23:59:05.062692Z","shell.execute_reply.started":"2023-10-01T23:59:05.049287Z","shell.execute_reply":"2023-10-01T23:59:05.061126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert smart meter usage datatype to float","metadata":{}},{"cell_type":"code","source":"# ~1 minute\nstart_time = time.time()\nd.iloc[:, 2] = pd.to_numeric(d.iloc[:, 2], errors='coerce')\nprint('%s seconds' % (time.time() - start_time))\n\n# rename usage column for easier reference\nd.rename(columns={d.columns[2]: 'KWHperHH'}, inplace=True)\nd.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:59:05.064615Z","iopub.execute_input":"2023-10-01T23:59:05.065069Z","iopub.status.idle":"2023-10-01T23:59:57.363487Z","shell.execute_reply.started":"2023-10-01T23:59:05.065032Z","shell.execute_reply":"2023-10-01T23:59:57.362369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set timestamp as the index\nstart_time = time.time()\nd.set_index('tstp')\nprint('%s seconds' % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:59:57.365174Z","iopub.execute_input":"2023-10-01T23:59:57.365572Z","iopub.status.idle":"2023-10-01T23:59:59.481686Z","shell.execute_reply.started":"2023-10-01T23:59:57.365540Z","shell.execute_reply":"2023-10-01T23:59:59.480421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Identify and handle duplicates in the smart meter data","metadata":{}},{"cell_type":"code","source":"# about 1.5 minutes\nstart_time = time.time()\ndupes = d[d.duplicated()]\nprint('dupes', dupes)\nprint('dupes.index', dupes.index)\nd.drop(index=dupes.index, inplace=True)\nprint('%s seconds' % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:59:59.483320Z","iopub.execute_input":"2023-10-01T23:59:59.483706Z","iopub.status.idle":"2023-10-02T00:01:42.117693Z","shell.execute_reply.started":"2023-10-01T23:59:59.483677Z","shell.execute_reply":"2023-10-02T00:01:42.115500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set index for the usage data to the timestamp column.  Is this necessary?  Can't remember why\nstart_time = time.time()\nd.set_index('tstp')\nd.info()\nprint('%s seconds' % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:01:42.121513Z","iopub.execute_input":"2023-10-02T00:01:42.122062Z","iopub.status.idle":"2023-10-02T00:01:46.069028Z","shell.execute_reply.started":"2023-10-02T00:01:42.122032Z","shell.execute_reply":"2023-10-02T00:01:46.061284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check what is gobbling RAM\nfor name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(\n                          locals().items())), key= lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:01:46.078298Z","iopub.execute_input":"2023-10-02T00:01:46.079079Z","iopub.status.idle":"2023-10-02T00:02:08.890555Z","shell.execute_reply.started":"2023-10-02T00:01:46.079040Z","shell.execute_reply":"2023-10-02T00:02:08.888610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize smart meter dataset to analyze for quality, completenes and othe insights","metadata":{}},{"cell_type":"code","source":"import seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:02:08.892487Z","iopub.execute_input":"2023-10-02T00:02:08.893354Z","iopub.status.idle":"2023-10-02T00:02:08.900052Z","shell.execute_reply.started":"2023-10-02T00:02:08.893317Z","shell.execute_reply":"2023-10-02T00:02:08.898455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## grab a random sample of 2% of meters for visualization and analysis","metadata":{}},{"cell_type":"code","source":"rng = np.random.default_rng(randomState)\n# random_state = np.random.RandomState(randomState)\nsampleMeters = rng.choice(d.LCLid.unique(), size=int(len(d.LCLid.unique())*0.02), replace=False)\n# sampleMeters = np.random.choice(d.LCLid.unique(), size=int(len(d.LCLid.unique())*0.02), replace=False, random_state=random_state)\nprint('sampleMeters:\\n', sampleMeters)\nsample = d[d['LCLid'].isin(sampleMeters)]\nprint('sample:\\n', sample)\nprint(sample.describe())\n# print(sample.info())","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:02:08.901581Z","iopub.execute_input":"2023-10-02T00:02:08.902938Z","iopub.status.idle":"2023-10-02T00:02:33.272282Z","shell.execute_reply.started":"2023-10-02T00:02:08.902871Z","shell.execute_reply":"2023-10-02T00:02:33.270491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Heatmap to visualize meter read coverage and completeness","metadata":{}},{"cell_type":"code","source":"# visualize meter read coverage and completeness\n# using a random sample of 2% of meters\nplt.subplots(figsize=(20,15))\npivot_table = pd.pivot_table(sample, columns='tstp', index='LCLid', values='KWHperHH')\nsns.heatmap(pivot_table)\nplt.savefig('meter data heatmap.png', format='png')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:02:33.273886Z","iopub.execute_input":"2023-10-02T00:02:33.274330Z","iopub.status.idle":"2023-10-02T00:02:51.189033Z","shell.execute_reply.started":"2023-10-02T00:02:33.274288Z","shell.execute_reply":"2023-10-02T00:02:51.187783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations from Heatmap...\n* several houses start producing load part-way through the period\n    - eg MAC004221, MAC004248\n    \n    \n* several houses stop producing part-way through the period\n    - eg MAC004226, MAC004257\n    \n\n* most houses have at least one \"gap\" in their data (visible as white lines)\n\n\n* several houses stand out as having significantly higher average load than others\n    - eg MAC004225, MAC004249","metadata":{}},{"cell_type":"markdown","source":"## identify and remove smart meter readings not exactly on the half-hour","metadata":{}},{"cell_type":"code","source":"# identify and remove records not exactly on the half-hour\n\nstart_time = time.time()\n\noffRecs = d.query(\"tstp.dt.minute not in (0,30) or tstp.dt.second != 0\")\n# aggLoad[\"DateTime\"].dt.hour > 30\nprint('Records not exactly on the half-hour:\\n ', offRecs)\nprint(offRecs.info())\n\n# delete records not exactly on the half-hour\nd.drop(offRecs.index, inplace=True)\n\nprint('%s seconds' % (time.time() - start_time))\n\noffRecs = d.query(\"tstp.dt.minute not in (0,30) or tstp.dt.second != 0\")\nprint('Records not exactly on the half-hour: ', offRecs)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:02:51.190494Z","iopub.execute_input":"2023-10-02T00:02:51.190885Z","iopub.status.idle":"2023-10-02T00:04:09.006413Z","shell.execute_reply.started":"2023-10-02T00:02:51.190854Z","shell.execute_reply":"2023-10-02T00:04:09.003659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:04:09.008564Z","iopub.execute_input":"2023-10-02T00:04:09.009812Z","iopub.status.idle":"2023-10-02T00:04:09.022265Z","shell.execute_reply.started":"2023-10-02T00:04:09.009762Z","shell.execute_reply":"2023-10-02T00:04:09.021066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check what is gobbling RAM\nfor name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(\n                          locals().items())), key= lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:04:09.023950Z","iopub.execute_input":"2023-10-02T00:04:09.024691Z","iopub.status.idle":"2023-10-02T00:04:33.374655Z","shell.execute_reply.started":"2023-10-02T00:04:09.024640Z","shell.execute_reply":"2023-10-02T00:04:33.373135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fill gaps in the smart meter data using interpolation\n\nOur heatmap above shows lots of gaps (small white vertical lines), and we'll fill those gaps using interpolation","metadata":{}},{"cell_type":"markdown","source":"### First step of filling these gaps is to create NaN records where records are missing\n\nThen we can fill these gas with interpolation","metadata":{}},{"cell_type":"code","source":"# First step of interpolation is to create NaN records where records are missing\n# about 2 minutes\nd.sort_values(by=['tstp'], inplace=True)\nd.set_index('tstp', inplace=True)\nd.index.rename('tstp', inplace=True)\n\nstart_time = time.time()\n# resample to create NaN records where records are missing\nd = d.groupby('LCLid')\\\n                .resample('30Min')\\\n                .mean()\n\n# fill the gaps with interpolation\nd['KWHperHH'] = d['KWHperHH'].interpolate()\nd.reset_index(inplace=True)\n\nprint('%s seconds' % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:04:33.376142Z","iopub.execute_input":"2023-10-02T00:04:33.376492Z","iopub.status.idle":"2023-10-02T00:07:41.206511Z","shell.execute_reply.started":"2023-10-02T00:04:33.376450Z","shell.execute_reply":"2023-10-02T00:07:41.204723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check the meter data heatmap to see if gaps have been filled","metadata":{}},{"cell_type":"code","source":"# visualize after interpolating missing values\nd.info()\nsample = d[d['LCLid'].isin(sampleMeters)]\npivot_table = pd.pivot_table(sample, columns='tstp', index='LCLid', values='KWHperHH')\nplt.subplots(figsize=(20,15))\n\nsns.heatmap(pivot_table)\nplt.savefig('meter data heatmap gaps filled.png', format='png')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:07:41.215503Z","iopub.execute_input":"2023-10-02T00:07:41.216864Z","iopub.status.idle":"2023-10-02T00:08:05.012294Z","shell.execute_reply.started":"2023-10-02T00:07:41.216821Z","shell.execute_reply":"2023-10-02T00:08:05.010492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize zeros in the dataset using heatmap\n\nI'm always curious to understand zeros in a dataset, and whether they are legitimate zero values, or indicate a data quality problem.","metadata":{}},{"cell_type":"code","source":"# visualize zeros in the dataset\nstart_time = time.time()\nsample = d[d['LCLid'].isin(sampleMeters)]\nsample['ZeroKWHperHH'] = sample['KWHperHH'] == 0\npivot_table = pd.pivot_table(sample, columns='tstp', index='LCLid', values='ZeroKWHperHH')\nprint('%s seconds' % (time.time() - start_time))\nplt.subplots(figsize=(20,15))\n\nsns.heatmap(pivot_table)\nplt.savefig('meter data heatmap zeros.png', format='png')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:08:05.016223Z","iopub.execute_input":"2023-10-02T00:08:05.018312Z","iopub.status.idle":"2023-10-02T00:08:28.454395Z","shell.execute_reply.started":"2023-10-02T00:08:05.018224Z","shell.execute_reply":"2023-10-02T00:08:28.452857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a snapshot of data\nstart_time = time.time()\nd.to_csv('/kaggle/working/MeterDataFinal.csv',index=False)\nprint('%s seconds' % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:17:00.253802Z","iopub.execute_input":"2023-10-02T00:17:00.254241Z","iopub.status.idle":"2023-10-02T00:19:41.962746Z","shell.execute_reply.started":"2023-10-02T00:17:00.254210Z","shell.execute_reply":"2023-10-02T00:19:41.960463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Obervation\n\nThere are a handful of households that account all the zero value meter reads: MAC004233, MAC004226, MAC004267","metadata":{}},{"cell_type":"code","source":"# investigate the meters with zero reads\nMAC005127 = sample.query(\"LCLid == 'MAC005127'\")\n\nfig, ax = plt.subplots(4,figsize=(20,9))\n\n# plot whole ~2 years\nax[0].plot(MAC005127.tstp, MAC005127.KWHperHH)\nax[0].plot(MAC005127.tstp, MAC005127.ZeroKWHperHH)\nax[0].set(ylabel='KWH/hh',\n       title='Load from one Household MAC004233 with lots of zero values')\nplt.tick_params(rotation=45)\nax[0].grid()\n\n# zoom in\nax[1].plot(MAC005127.tstp[11000:15000], MAC005127.KWHperHH[11000:15000])\nax[1].plot(MAC005127.tstp[11000:15000], MAC005127.ZeroKWHperHH[11000:15000])\nax[1].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[1].grid()\n\n# zoom in more...\nax[2].plot(MAC005127.tstp[13000:13500], MAC005127.KWHperHH[13000:13500])\nax[2].plot(MAC005127.tstp[13000:13500], MAC005127.ZeroKWHperHH[13000:13500])\nax[2].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[2].grid()\n\n# zoom in to a different part of the series...\nax[3].plot(MAC005127.tstp[25000:25500], MAC005127.KWHperHH[25000:25500])\nax[3].plot(MAC005127.tstp[25000:25500], MAC005127.ZeroKWHperHH[25000:25500])\nax[3].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[3].grid()\n\nfig.savefig(\"MAC005127.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:16:51.258764Z","iopub.status.idle":"2023-10-02T00:16:51.259194Z","shell.execute_reply.started":"2023-10-02T00:16:51.258992Z","shell.execute_reply":"2023-10-02T00:16:51.259010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\nThe zeros for MAC004233 seem legit - leaving them in","metadata":{}},{"cell_type":"code","source":"# investigate the meters with zero reads\nMAC002304 = sample.query(\"LCLid == 'MAC002304'\")\nfig, ax = plt.subplots(4,figsize=(20,9))\n\n# plot whole ~2 years\nax[0].plot(MAC002304.tstp, MAC002304.KWHperHH)\nax[0].plot(MAC002304.tstp, MAC002304.ZeroKWHperHH)\nax[0].set(ylabel='KWH/hh',\n       title='Load from one Household MAC002304 with lots of zero values')\nplt.tick_params(rotation=45)\nax[0].grid()\n\n# zoom in\nax[1].plot(MAC002304.tstp[17000:21000], MAC002304.KWHperHH[17000:21000])\nax[1].plot(MAC002304.tstp[17000:21000], MAC002304.ZeroKWHperHH[17000:21000])\nax[1].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[1].grid()\n\n# zoom in more...\nax[2].plot(MAC002304.tstp[19300:19800], MAC002304.KWHperHH[19300:19800])\nax[2].plot(MAC002304.tstp[19300:19800], MAC002304.ZeroKWHperHH[19300:19800])\nax[2].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[2].grid()\n\n# zoom in to a different part of the series...\nax[3].plot(MAC002304.tstp[25000:25500], MAC002304.KWHperHH[25000:25500])\nax[3].plot(MAC002304.tstp[25000:25500], MAC002304.ZeroKWHperHH[25000:25500])\nax[3].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[3].grid()\n\nfig.savefig(\"MAC002304.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:16:51.262457Z","iopub.status.idle":"2023-10-02T00:16:51.263321Z","shell.execute_reply.started":"2023-10-02T00:16:51.263077Z","shell.execute_reply":"2023-10-02T00:16:51.263105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation\n\nThe zeros for MAC004233 seem legit - leaving them in\n","metadata":{}},{"cell_type":"code","source":"# visualize and handle outliers\n\n# minumum and maximum timestamp for each house\nprint(d.groupby('LCLid').max().sort_values('tstp'))\nprint(d.groupby('LCLid').min().sort_values('tstp'))\nprint(d.groupby('LCLid').count().sort_values('tstp'))\n\nprint(d.groupby('LCLid').agg(['min', 'max', 'count']))\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:16:51.265439Z","iopub.status.idle":"2023-10-02T00:16:51.265933Z","shell.execute_reply.started":"2023-10-02T00:16:51.265721Z","shell.execute_reply":"2023-10-02T00:16:51.265744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# which house has the highest peak load?\n\n# which house has the highest total aggregate load?\n\n# how variable / predictable is the timing of the peak load\n\n# how accurate is the next 24 hours forecast profile overall?\n\n# how accurate is the peak load forecast in next 24 hours?\n\n# normalize and standardize\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:16:51.267558Z","iopub.status.idle":"2023-10-02T00:16:51.268514Z","shell.execute_reply.started":"2023-10-02T00:16:51.268298Z","shell.execute_reply":"2023-10-02T00:16:51.268322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract one smartmeter for plotting\nsample = d.query(\"LCLid == 'MAC004233'\")\nsample","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:16:51.270060Z","iopub.status.idle":"2023-10-02T00:16:51.270490Z","shell.execute_reply.started":"2023-10-02T00:16:51.270300Z","shell.execute_reply":"2023-10-02T00:16:51.270321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize load profile for one household meter\nfig, ax = plt.subplots()\nax.plot(sample.iloc[100:4500,1], sample.iloc[100:4500,2])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Load from one Household, June-September 2012')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Load from one Household, June-September 2012.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:16:51.272042Z","iopub.status.idle":"2023-10-02T00:16:51.272726Z","shell.execute_reply.started":"2023-10-02T00:16:51.272483Z","shell.execute_reply":"2023-10-02T00:16:51.272511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set index for the sample\nsample.set_index('tstp')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:16:51.273917Z","iopub.status.idle":"2023-10-02T00:16:51.274560Z","shell.execute_reply.started":"2023-10-02T00:16:51.274344Z","shell.execute_reply":"2023-10-02T00:16:51.274369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA: Visualize daily average load for each meter and all meters...","metadata":{}},{"cell_type":"code","source":"# calculate average daily load profile for all meters...\n# abut 1.5 minutes\n\nstart_time = time.time()\navgLoadProfile = pd.DataFrame(d.groupby([d['tstp'].dt.hour, d['tstp'].dt.minute]).KWHperHH.mean())\navgLoadProfile = avgLoadProfile.reset_index(names=['hour', 'minute'])\navgLoadProfile['labels'] = pd.to_datetime(avgLoadProfile['hour'].astype(str) + ':' + avgLoadProfile['minute'].astype(str), format='%H:%M').dt.time\nprint('%s seconds' % (time.time() - start_time))\n\n# print(avgLoadProfile.info())\n# print(avgLoadProfile)\n\nfig, ax = plt.subplots(figsize=(10,7))\n\nax.set_xticks(avgLoadProfile.index, avgLoadProfile.labels)\n\nax.set(xlabel='time (HH:MI)', ylabel='KWH/hh',\n       title='Average Household 24 hour load profile')\n\n# calculate average daily load for each meter...\nstart_time = time.time()\navgLoadProfileEachMeter = pd.DataFrame(d.groupby(['LCLid', d['tstp'].dt.hour, d['tstp'].dt.minute]).agg({'KWHperHH': 'mean'}))\navgLoadProfileEachMeter = avgLoadProfileEachMeter.reset_index(names=['LCLid', 'hour', 'minute'])\nprint('%s seconds' % (time.time() - start_time))\n# print(avgLoadProfileEachMeter.info())\n# print(avgLoadProfileEachMeter)\n\n# plot every sample meter\nfor meter in sampleMeters:\n    # print(meter)\n    ax.plot(avgLoadProfileEachMeter.loc[avgLoadProfileEachMeter['LCLid'] == meter].index % 48, \n            avgLoadProfileEachMeter.loc[avgLoadProfileEachMeter['LCLid'] == meter].KWHperHH,\n           color='grey')\n\n# plot the average\nax.plot(avgLoadProfile.index, avgLoadProfile.KWHperHH, linewidth=5)\n\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Avg 24hr Load Profile every meter.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:16:51.275799Z","iopub.status.idle":"2023-10-02T00:16:51.276429Z","shell.execute_reply.started":"2023-10-02T00:16:51.276230Z","shell.execute_reply":"2023-10-02T00:16:51.276254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the sum of all loads for each timestamp using `groupby()` and `agg()`\nstart_time = time.time()\n# aggLoad = d.groupby('tstp')['KWHperHH'].agg('sum')\naggLoad = pd.DataFrame(d.groupby('tstp')['KWHperHH'].agg({'sum', 'count'}))\naggLoad.reset_index(inplace=True)\naggLoad.columns = ['tstp', 'AggregateLoad', 'numMeters']\nprint('%s seconds' % (time.time() - start_time))\n\nprint(aggLoad)\nprint(aggLoad.describe())\nprint(aggLoad.info())","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:34:22.754671Z","iopub.execute_input":"2023-10-02T00:34:22.756389Z","iopub.status.idle":"2023-10-02T00:34:29.289853Z","shell.execute_reply.started":"2023-10-02T00:34:22.756333Z","shell.execute_reply":"2023-10-02T00:34:29.287864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggLoad.sort_values(by=['tstp'], inplace=True)\naggLoad.set_index('tstp', inplace=True)\naggLoad.index.rename('DateTimeIndex', inplace=True)\naggLoad.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:35:06.755618Z","iopub.execute_input":"2023-10-02T00:35:06.756064Z","iopub.status.idle":"2023-10-02T00:35:06.776984Z","shell.execute_reply.started":"2023-10-02T00:35:06.756032Z","shell.execute_reply":"2023-10-02T00:35:06.775583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggLoad['DateTime'] = aggLoad.index\naggLoad.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:35:19.927268Z","iopub.execute_input":"2023-10-02T00:35:19.927789Z","iopub.status.idle":"2023-10-02T00:35:19.950663Z","shell.execute_reply.started":"2023-10-02T00:35:19.927751Z","shell.execute_reply":"2023-10-02T00:35:19.949356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inspect and fix records with zero load\n# start with the aggregated records with zero load\nAggZeros = aggLoad.query(\"AggregateLoad == 0\")\nAggZeros\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:35:26.553720Z","iopub.execute_input":"2023-10-02T00:35:26.554153Z","iopub.status.idle":"2023-10-02T00:35:26.571388Z","shell.execute_reply.started":"2023-10-02T00:35:26.554125Z","shell.execute_reply":"2023-10-02T00:35:26.569702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation: Some of the timestamps are not exactly on the half-hour\nQuestion: How many of the timestamps are not exactly on the half-hour?","metadata":{}},{"cell_type":"code","source":"# inspect and fix records not exactly on the half-hour\noffRecs = aggLoad.query(\"DateTime.dt.minute not in (0,30) or DateTime.dt.second != 0\")\n# aggLoad[\"DateTime\"].dt.hour > 30\nprint('Records not exactly on the half-hour: ', offRecs)\nprint(offRecs.info())\n\n# delete records not exactly on the half-hour\naggLoad = aggLoad.drop(offRecs.index)\n\noffRecs = aggLoad.query(\"DateTime.dt.minute not in (0,30) or DateTime.dt.second != 0\")\nprint('Records not exactly on the half-hour: ', offRecs)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:35:30.819063Z","iopub.execute_input":"2023-10-02T00:35:30.819615Z","iopub.status.idle":"2023-10-02T00:35:30.867864Z","shell.execute_reply.started":"2023-10-02T00:35:30.819561Z","shell.execute_reply":"2023-10-02T00:35:30.866461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the regularity of the observations (time between observations)\n# print(pd.infer_freq(train_data.DateTime))\naggLoad.index.to_series().diff().value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:35:39.470764Z","iopub.execute_input":"2023-10-02T00:35:39.471289Z","iopub.status.idle":"2023-10-02T00:35:39.485569Z","shell.execute_reply.started":"2023-10-02T00:35:39.471250Z","shell.execute_reply":"2023-10-02T00:35:39.484560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate moving average and stddev\nwindow_size = int(len(aggLoad.AggregateLoad) / 10)\nprint(window_size)\n\naggLoadMovingStdev = aggLoad.AggregateLoad.rolling(window_size).std()\naggLoadMovingStdev.columns = ['MovingStdev']\n# aggLoadMovingStdev.columns.values[0] = 'MovingStdev'\n\naggLoadMovingAvg = aggLoad.AggregateLoad.rolling(window_size).mean()\naggLoadMovingAvg.columns = ['MovingAvg']\n\nprint('aggLoadMovingStdev:\\n', aggLoadMovingStdev)\nprint(aggLoadMovingStdev.info())\nprint('aggLoadMovingAvg:\\n', aggLoadMovingAvg)\nprint(aggLoadMovingAvg.info())\n\n# aggLoad['MovingStdev'] = aggLoad.AggregateLoad.rolling(window_size).std()\n# aggLoad['MovingAvg'] = aggLoad.AggregateLoad.rolling(window_size).mean()\n\n# print('aggLoad.MovingStdev:\\n', aggLoad.MovingStdev)\n# print('aggLoad.MovingAvg:\\n', aggLoad.MovingAvg)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:35:44.448229Z","iopub.execute_input":"2023-10-02T00:35:44.448768Z","iopub.status.idle":"2023-10-02T00:35:44.481170Z","shell.execute_reply.started":"2023-10-02T00:35:44.448731Z","shell.execute_reply":"2023-10-02T00:35:44.479639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(aggLoad)\n\nfig, ax = plt.subplots(figsize=(20,7))\nax.plot(aggLoad.DateTime, aggLoad.AggregateLoad)\n# ax.plot(aggLoad.DateTime, aggLoad.MovingAvg, linewidth=3)\n# ax.plot(aggLoad.DateTime, aggLoad.MovingStdev, linewidth=3)\nax.plot(aggLoad.DateTime, aggLoadMovingAvg, linewidth=3)\nax.plot(aggLoad.DateTime, aggLoadMovingStdev, linewidth=3)\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load 2012-2014')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Aggregate Household load 2012-2014.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:36:09.917510Z","iopub.execute_input":"2023-10-02T00:36:09.919212Z","iopub.status.idle":"2023-10-02T00:36:10.740491Z","shell.execute_reply.started":"2023-10-02T00:36:09.919158Z","shell.execute_reply":"2023-10-02T00:36:10.739117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggLoad.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:36:20.893650Z","iopub.execute_input":"2023-10-02T00:36:20.894196Z","iopub.status.idle":"2023-10-02T00:36:20.909548Z","shell.execute_reply.started":"2023-10-02T00:36:20.894150Z","shell.execute_reply":"2023-10-02T00:36:20.908183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,7))\nax.plot(aggLoad.DateTime[10000:15000], aggLoad.AggregateLoad[10000:15000])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load June-August 2012')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Aggregate Household load June-August 2012.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:36:30.039368Z","iopub.execute_input":"2023-10-02T00:36:30.040309Z","iopub.status.idle":"2023-10-02T00:36:30.743903Z","shell.execute_reply.started":"2023-10-02T00:36:30.040268Z","shell.execute_reply":"2023-10-02T00:36:30.742955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,7))\nax.plot(aggLoad.DateTime[12000:13000], aggLoad.AggregateLoad[12000:13000])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Aggregate Household load one month.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:36:36.064698Z","iopub.execute_input":"2023-10-02T00:36:36.065543Z","iopub.status.idle":"2023-10-02T00:36:36.619671Z","shell.execute_reply.started":"2023-10-02T00:36:36.065502Z","shell.execute_reply":"2023-10-02T00:36:36.618195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,7))\nax.plot(aggLoad.DateTime[12500:12600], aggLoad.AggregateLoad[12500:12600])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load ~two days')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Aggregate Household load two days.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:36:40.568336Z","iopub.execute_input":"2023-10-02T00:36:40.568770Z","iopub.status.idle":"2023-10-02T00:36:41.103543Z","shell.execute_reply.started":"2023-10-02T00:36:40.568737Z","shell.execute_reply":"2023-10-02T00:36:41.102548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.plot(aggLoad.DateTime[12500:12550], aggLoad.AggregateLoad[12500:12550])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load (one day)')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Aggregate Household load (one day).png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:36:45.689687Z","iopub.execute_input":"2023-10-02T00:36:45.690193Z","iopub.status.idle":"2023-10-02T00:36:46.129209Z","shell.execute_reply.started":"2023-10-02T00:36:45.690158Z","shell.execute_reply":"2023-10-02T00:36:46.127921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggLoad.to_csv('/kaggle/working/aggLoadDataFinal.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:36:51.453083Z","iopub.execute_input":"2023-10-02T00:36:51.453996Z","iopub.status.idle":"2023-10-02T00:36:51.720534Z","shell.execute_reply.started":"2023-10-02T00:36:51.453952Z","shell.execute_reply":"2023-10-02T00:36:51.718941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggLoad = pd.read_csv('/kaggle/working/aggLoadDataFinal.csv', parse_dates=[\"DateTime\"])\naggLoad","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:36:58.044103Z","iopub.execute_input":"2023-10-02T00:36:58.045751Z","iopub.status.idle":"2023-10-02T00:36:58.116153Z","shell.execute_reply.started":"2023-10-02T00:36:58.045693Z","shell.execute_reply":"2023-10-02T00:36:58.115162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add features useful for time series\n!pip install scikit-learn\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import preprocessing","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:37:07.388387Z","iopub.execute_input":"2023-10-02T00:37:07.389897Z","iopub.status.idle":"2023-10-02T00:37:18.723358Z","shell.execute_reply.started":"2023-10-02T00:37:07.389852Z","shell.execute_reply":"2023-10-02T00:37:18.721380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add features useful for time series\nprint(aggLoad.info())\n# day of week\naggLoad[\"dayOfWeek\"] = aggLoad.DateTime.dt.dayofweek\n# day of year\naggLoad[\"dayOfYear\"] = aggLoad.DateTime.dt.dayofyear\n# minute of the day\naggLoad[\"minuteOfDay\"] = (aggLoad.DateTime.dt.hour * 60) + aggLoad.DateTime.dt.minute\n# number of meters\n\nprint(aggLoad.info())","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:37:24.967234Z","iopub.execute_input":"2023-10-02T00:37:24.967784Z","iopub.status.idle":"2023-10-02T00:37:25.016232Z","shell.execute_reply.started":"2023-10-02T00:37:24.967745Z","shell.execute_reply":"2023-10-02T00:37:25.014682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Join load data and weather data\nmergeData = pd.merge(aggLoad, weatherUpsample, on='DateTime', copy=False)\nprint(mergeData.info())\nmergeData\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:37:43.189158Z","iopub.execute_input":"2023-10-02T00:37:43.189626Z","iopub.status.idle":"2023-10-02T00:37:43.250757Z","shell.execute_reply.started":"2023-10-02T00:37:43.189565Z","shell.execute_reply":"2023-10-02T00:37:43.249780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move first column to the Last\n# df = pd.DataFrame(mergeData)\ndf = mergeData\ntemp_cols=df.columns.tolist()\nnew_cols=temp_cols[1:] + temp_cols[0:1]\nmergeData=df[new_cols]\nprint(mergeData)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:37:56.066690Z","iopub.execute_input":"2023-10-02T00:37:56.067107Z","iopub.status.idle":"2023-10-02T00:37:56.084219Z","shell.execute_reply.started":"2023-10-02T00:37:56.067078Z","shell.execute_reply":"2023-10-02T00:37:56.082717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas_profiling import ProfileReport\n\nprofile = ProfileReport(mergeData, tsmode=True, sortby=\"DateTime\")\nprofile.to_file('mergeData profile_report.html')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:38:04.389179Z","iopub.execute_input":"2023-10-02T00:38:04.389626Z","iopub.status.idle":"2023-10-02T00:39:50.482965Z","shell.execute_reply.started":"2023-10-02T00:38:04.389567Z","shell.execute_reply":"2023-10-02T00:39:50.482073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoData = mergeData.copy() # keep a copy for the autogluon AutoML","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:43:45.739078Z","iopub.execute_input":"2023-10-02T00:43:45.739706Z","iopub.status.idle":"2023-10-02T00:43:45.748986Z","shell.execute_reply.started":"2023-10-02T00:43:45.739667Z","shell.execute_reply":"2023-10-02T00:43:45.747086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove the dateTime feature from the dataset\n\nprint(mergeData)\nmergeData = mergeData.drop(columns=['DateTime'])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:44:18.077437Z","iopub.execute_input":"2023-10-02T00:44:18.077924Z","iopub.status.idle":"2023-10-02T00:44:21.285787Z","shell.execute_reply.started":"2023-10-02T00:44:18.077890Z","shell.execute_reply":"2023-10-02T00:44:21.283727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoData.to_csv('/kaggle/working/autoDataFinal.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:46:08.313960Z","iopub.execute_input":"2023-10-02T00:46:08.314511Z","iopub.status.idle":"2023-10-02T00:46:08.601657Z","shell.execute_reply.started":"2023-10-02T00:46:08.314478Z","shell.execute_reply":"2023-10-02T00:46:08.600089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mergeData.to_csv('/kaggle/working/mergeDataFinal.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:46:11.570292Z","iopub.execute_input":"2023-10-02T00:46:11.570796Z","iopub.status.idle":"2023-10-02T00:46:11.836913Z","shell.execute_reply.started":"2023-10-02T00:46:11.570760Z","shell.execute_reply":"2023-10-02T00:46:11.835400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mergeData = pd.read_csv('/kaggle/working/mergeDataFinal.csv')\nmergeData","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:46:15.660669Z","iopub.execute_input":"2023-10-02T00:46:15.661189Z","iopub.status.idle":"2023-10-02T00:46:15.727221Z","shell.execute_reply.started":"2023-10-02T00:46:15.661155Z","shell.execute_reply":"2023-10-02T00:46:15.726246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mergeData.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:46:26.912327Z","iopub.execute_input":"2023-10-02T00:46:26.912829Z","iopub.status.idle":"2023-10-02T00:46:26.930622Z","shell.execute_reply.started":"2023-10-02T00:46:26.912790Z","shell.execute_reply":"2023-10-02T00:46:26.929160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the time series data into train, test, and validation datasets\ntrain_size = int(len(mergeData) * 0.7)  # 70% for training\nval_size = int(len(mergeData) * 0.2)   # 20% for validation\ntest_size = len(mergeData) - val_size - train_size  # Remaining for testing\n\ntrain_data = mergeData[:train_size].copy()\nval_data = mergeData[train_size:train_size+val_size].copy()\ntest_data = mergeData[train_size+val_size:].copy()\n\nprint('train_data.head()', train_data.head())\nprint('val_data.head()', val_data.head())\nprint('test_data.head()', test_data.head())\nprint(train_data.info())\n\nnum_features = mergeData.shape[1]\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:46:32.248115Z","iopub.execute_input":"2023-10-02T00:46:32.248517Z","iopub.status.idle":"2023-10-02T00:46:32.276333Z","shell.execute_reply.started":"2023-10-02T00:46:32.248479Z","shell.execute_reply":"2023-10-02T00:46:32.275330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction_plot(testY, test_predict):\n      len_prediction=[x for x in range(len(testY))]\n      plt.figure(figsize=(20,5))\n      plt.plot(len_prediction, testY, marker='.', label=\"actual\")\n      plt.plot(len_prediction, test_predict, 'r', label=\"prediction\")\n      plt.tight_layout()\n      sns.despine(top=True)\n      plt.subplots_adjust(left=0.07)\n      plt.ylabel('KWH per half hour', size=15)\n      plt.xlabel('Time step', size=15)\n      plt.legend(fontsize=15)\n      plt.show();","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:46:40.273004Z","iopub.execute_input":"2023-10-02T00:46:40.274397Z","iopub.status.idle":"2023-10-02T00:46:40.284546Z","shell.execute_reply.started":"2023-10-02T00:46:40.274339Z","shell.execute_reply":"2023-10-02T00:46:40.283395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize the data\ntrain_mean = train_data.mean()\ntrain_std = train_data.std()\n\ntrain_data = (train_data - train_mean) / train_std\nval_data = (val_data - train_mean) / train_std\ntest_data = (test_data - train_mean) / train_std\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:46:45.108389Z","iopub.execute_input":"2023-10-02T00:46:45.108839Z","iopub.status.idle":"2023-10-02T00:46:45.130194Z","shell.execute_reply.started":"2023-10-02T00:46:45.108805Z","shell.execute_reply":"2023-10-02T00:46:45.128952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize distribution of the features\ndf_std = (mergeData - train_mean) / train_std\ndf_std = df_std.melt(var_name='Column', value_name='Normalized')\nplt.figure(figsize=(12, 6))\nax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n_ = ax.set_xticklabels(mergeData.keys(), rotation=90)\nplt.savefig('violin_plot.png', format='png')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:46:49.604032Z","iopub.execute_input":"2023-10-02T00:46:49.604558Z","iopub.status.idle":"2023-10-02T00:46:51.631955Z","shell.execute_reply.started":"2023-10-02T00:46:49.604520Z","shell.execute_reply":"2023-10-02T00:46:51.630673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WindowGenerator():\n    # https://www.tensorflow.org/tutorials/structured_data/time_series#1_indexes_and_offsets\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_data, val_df=val_data, test_df=test_data,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:47:20.850740Z","iopub.execute_input":"2023-10-02T00:47:20.851221Z","iopub.status.idle":"2023-10-02T00:47:20.861173Z","shell.execute_reply.started":"2023-10-02T00:47:20.851184Z","shell.execute_reply":"2023-10-02T00:47:20.860024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_window(self, features):\n    # https://www.tensorflow.org/tutorials/structured_data/time_series#2_split\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nWindowGenerator.split_window = split_window\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:47:24.851678Z","iopub.execute_input":"2023-10-02T00:47:24.852222Z","iopub.status.idle":"2023-10-02T00:47:24.860992Z","shell.execute_reply.started":"2023-10-02T00:47:24.852183Z","shell.execute_reply":"2023-10-02T00:47:24.859655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot(self, model=None, plot_col='AggregateLoad', max_subplots=3):\n    # https://www.tensorflow.org/tutorials/structured_data/time_series#3_plot\n  inputs, labels = self.example\n  plt.figure(figsize=(12, 8))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    if model is not None:\n      predictions = model(inputs)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', edgecolors='k', label='Predictions',\n                  c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [h]')\n\nWindowGenerator.plot = plot","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:47:27.989845Z","iopub.execute_input":"2023-10-02T00:47:27.990331Z","iopub.status.idle":"2023-10-02T00:47:28.002840Z","shell.execute_reply.started":"2023-10-02T00:47:27.990295Z","shell.execute_reply":"2023-10-02T00:47:28.000215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_dataset(self, data):\n    # https://www.tensorflow.org/tutorials/structured_data/time_series#4_create_tfdatadatasets\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=True,\n      seed=randomState,\n      batch_size=32,)\n\n  ds = ds.map(self.split_window)\n\n  return ds\n\nWindowGenerator.make_dataset = make_dataset","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:47:31.758694Z","iopub.execute_input":"2023-10-02T00:47:31.759268Z","iopub.status.idle":"2023-10-02T00:47:31.768434Z","shell.execute_reply.started":"2023-10-02T00:47:31.759230Z","shell.execute_reply":"2023-10-02T00:47:31.766766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.tensorflow.org/tutorials/structured_data/time_series#4_create_tfdatadatasets\n@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:47:35.592909Z","iopub.execute_input":"2023-10-02T00:47:35.593923Z","iopub.status.idle":"2023-10-02T00:47:35.602504Z","shell.execute_reply.started":"2023-10-02T00:47:35.593884Z","shell.execute_reply":"2023-10-02T00:47:35.601379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare data for one-shot multi-step\nOUT_STEPS = 48 # 24 hour forecast\nIN_STEPS = 336 # look back 1 week\nmulti_window = WindowGenerator(input_width=IN_STEPS,\n                               label_width=OUT_STEPS,\n                               shift=OUT_STEPS)\n\nmulti_window.plot()\nmulti_window","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:48:37.333980Z","iopub.execute_input":"2023-10-02T00:48:37.334542Z","iopub.status.idle":"2023-10-02T00:48:38.311277Z","shell.execute_reply.started":"2023-10-02T00:48:37.334507Z","shell.execute_reply":"2023-10-02T00:48:38.309497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use a naive persistence model as baseline to compare more sophisticated models\nUse a 1 week persistence\n\nGeorgios Tziolis, Chrysovalantis Spanias, Maria Theodoride, Spyros Theocharides, Javier Lopez-Lorente, Andreas Livera, George Makrides, George E. Georghiou,\n\nShort-term electric net load forecasting for solar-integrated distribution systems based on Bayesian neural networks and statistical post-processing,\n\nEnergy,\nVolume 271,\n2023,\n127018,\nISSN 0360-5442,\n\nhttps://doi.org/10.1016/j.energy.2023.127018.","metadata":{}},{"cell_type":"code","source":"# capture performnce of models\nmulti_val_performance = {}\nmulti_performance = {}","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:48:51.658624Z","iopub.execute_input":"2023-10-02T00:48:51.659080Z","iopub.status.idle":"2023-10-02T00:48:51.666577Z","shell.execute_reply.started":"2023-10-02T00:48:51.659047Z","shell.execute_reply":"2023-10-02T00:48:51.664678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.tensorflow.org/tutorials/structured_data/time_series#linear_model\nMAX_EPOCHS = 50\n\ndef compile_and_fit(model, window, patience=5):\n  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\n  checkpoint_filepath = '/tmp/' + model.name + '/checkpoint'\n  checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_filepath, \n                    monitor=\"val_loss\", mode=\"min\", \n                    save_best_only=True, verbose=1)\n    \n  history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping, checkpoint])\n\n  model.load_weights(checkpoint_filepath)\n    \n  return history\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:48:51.671191Z","iopub.execute_input":"2023-10-02T00:48:51.672616Z","iopub.status.idle":"2023-10-02T00:48:51.683159Z","shell.execute_reply.started":"2023-10-02T00:48:51.672543Z","shell.execute_reply":"2023-10-02T00:48:51.681500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Naive 1 week persistence model\nNaiveForecast = mergeData.AggregateLoad.copy()\n\nOneWeekNPeriods = 48 * 7\n\nNaiveForecast[:OneWeekNPeriods] = np.nan\n\nfor i in range(OneWeekNPeriods, len(mergeData.AggregateLoad)):\n    NaiveForecast[i] = mergeData.AggregateLoad[i - OneWeekNPeriods]\n\nprint(NaiveForecast.info())\nprint(NaiveForecast.describe())","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:48:51.686140Z","iopub.execute_input":"2023-10-02T00:48:51.686742Z","iopub.status.idle":"2023-10-02T00:48:52.483435Z","shell.execute_reply.started":"2023-10-02T00:48:51.686693Z","shell.execute_reply":"2023-10-02T00:48:52.481853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize naive forecast\nprediction_plot(mergeData.AggregateLoad, NaiveForecast)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:48:52.485868Z","iopub.execute_input":"2023-10-02T00:48:52.486204Z","iopub.status.idle":"2023-10-02T00:48:53.181492Z","shell.execute_reply.started":"2023-10-02T00:48:52.486179Z","shell.execute_reply":"2023-10-02T00:48:53.179751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize naive forecast and actuals for first day of the test dataset\nprediction_plot(mergeData.AggregateLoad[train_size+test_size:train_size+test_size+OUT_STEPS], NaiveForecast[train_size+test_size:train_size+test_size+OUT_STEPS])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:48:53.183964Z","iopub.execute_input":"2023-10-02T00:48:53.184418Z","iopub.status.idle":"2023-10-02T00:48:53.580958Z","shell.execute_reply.started":"2023-10-02T00:48:53.184384Z","shell.execute_reply":"2023-10-02T00:48:53.578984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The average offer price for balancing actions  to align supply and demand  between the start of September and early January was 287 a MWh. Data from Elexon, which oversees the market, shows Rye House submitted the 20 highest winter bids  between 5,000 and 6,000 a MWh  for varying volumes of power on 12 December, setting new records\nhttps://www.theguardian.com/business/2023/jan/29/gas-fired-plants-uk-lights-on-cost-profits-energy-crisis","metadata":{}},{"cell_type":"code","source":"# Area Under Curve\nfrom scipy.integrate import simpson\nfrom numpy import trapz\n\n\n# The y values.  A numpy array is used here,\n# but a python list could also be used.\n# y = np.array([5, 20, 4, 18, 19, 18, 7, 4])\nyhat = NaiveForecast[train_size+test_size:train_size+test_size+OUT_STEPS]\n\n# Compute the area using the composite trapezoidal rule.\nyhatArea = trapz(yhat, dx=5)\nprint(\"yhat area =\", yhatArea)\n\n# Compute the area using the composite Simpson's rule.\nyhatArea = simpson(yhat, dx=5)\nprint(\"yhat area =\", yhatArea)\n\nyActual = mergeData.AggregateLoad[train_size+test_size:train_size+test_size+OUT_STEPS]\n\n# Compute the area using the composite trapezoidal rule.\nyActualArea = trapz(yActual, dx=5)\nprint(\"yActualArea =\", yActualArea)\n\n# Compute the area using the composite Simpson's rule.\nyActualArea = simpson(yActual, dx=5)\nprint(\"yActualArea =\", yActualArea)\n\n# Calculate the area difference between the two curves...\nAreaDifference = yhatArea - yActualArea\nprint(\"AreaDifference =\", AreaDifference)\n\n# calculate price at 287 a MWh\ncost = (AreaDifference / 1000) * 287\nprint(\"cost for 1 day for 5k homes =\", cost)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:48:53.584164Z","iopub.execute_input":"2023-10-02T00:48:53.585483Z","iopub.status.idle":"2023-10-02T00:48:53.597811Z","shell.execute_reply.started":"2023-10-02T00:48:53.585429Z","shell.execute_reply":"2023-10-02T00:48:53.595770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The population of England was 56,489,800 in 2021\n24.9 million dwellings in England\n\n","metadata":{}},{"cell_type":"code","source":"# Scale up costs\ncost = cost * (56000000/5000)\nprint(\"cost for 1 day for 5k homes =\", cost)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:48:53.599565Z","iopub.execute_input":"2023-10-02T00:48:53.599984Z","iopub.status.idle":"2023-10-02T00:48:53.623572Z","shell.execute_reply.started":"2023-10-02T00:48:53.599955Z","shell.execute_reply":"2023-10-02T00:48:53.621945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# normalize the naive \n# (NaiveForecast - train_mean) / train_std\nNaiveForecastNormed = NaiveForecast.transform(lambda x: (x - train_mean) / train_std)\nNaiveForecastNormed.describe()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:48:53.626055Z","iopub.execute_input":"2023-10-02T00:48:53.626458Z","iopub.status.idle":"2023-10-02T00:49:01.222432Z","shell.execute_reply.started":"2023-10-02T00:48:53.626427Z","shell.execute_reply":"2023-10-02T00:49:01.221100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize naive forecast and actuals for first day of the test dataset\nmergeDataNormed = (mergeData - train_mean) / train_std\nprediction_plot(mergeDataNormed.AggregateLoad[train_size+test_size:train_size+test_size+OUT_STEPS], NaiveForecastNormed.AggregateLoad[train_size+test_size:train_size+test_size+OUT_STEPS])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:49:01.224350Z","iopub.execute_input":"2023-10-02T00:49:01.224759Z","iopub.status.idle":"2023-10-02T00:49:01.639138Z","shell.execute_reply.started":"2023-10-02T00:49:01.224727Z","shell.execute_reply":"2023-10-02T00:49:01.637990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(mergeDataNormed.AggregateLoad[train_size:train_size+val_size])\nprint(NaiveForecastNormed.AggregateLoad[train_size:train_size+val_size])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:49:01.640572Z","iopub.execute_input":"2023-10-02T00:49:01.641142Z","iopub.status.idle":"2023-10-02T00:49:01.652059Z","shell.execute_reply.started":"2023-10-02T00:49:01.641109Z","shell.execute_reply":"2023-10-02T00:49:01.650354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate error for naive model (Normed)\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n\n# calculate error for naive model on validation set\nvalNaiveMAE = mean_absolute_error(mergeDataNormed.AggregateLoad[train_size:train_size+val_size], NaiveForecastNormed.AggregateLoad[train_size:train_size+val_size])\n\n# calculate error for naive model on test set\ntestNaiveMAE = mean_absolute_error(mergeDataNormed.AggregateLoad[train_size+val_size:], NaiveForecastNormed.AggregateLoad[train_size+val_size:])\n\nprint('valNaiveMAE: ', valNaiveMAE)\nprint('testNaiveMAE: ', testNaiveMAE)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:49:01.653825Z","iopub.execute_input":"2023-10-02T00:49:01.654397Z","iopub.status.idle":"2023-10-02T00:49:01.677097Z","shell.execute_reply.started":"2023-10-02T00:49:01.654359Z","shell.execute_reply":"2023-10-02T00:49:01.675381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate error for naive model (not normed)\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\nprint('Naive Root Mean Squared Error(RMSE): %.2f; Naive Mean Absolute Error(MAE) : %.2f; Naive Mean Absolute Percantage Error(MAPE) : %.2f '\n      % (np.sqrt(mean_squared_error(mergeData.AggregateLoad[OneWeekNPeriods:], NaiveForecast[OneWeekNPeriods:])),\n         mean_absolute_error(mergeData.AggregateLoad[OneWeekNPeriods:], NaiveForecast[OneWeekNPeriods:]),\n         mean_absolute_percentage_error(mergeData.AggregateLoad[OneWeekNPeriods:], NaiveForecast[OneWeekNPeriods:])))\n\n# calculate error for naive model on validation set\nvalNaiveMAE = mean_absolute_error(mergeData.AggregateLoad[train_size:train_size+val_size], NaiveForecast[train_size:train_size+val_size])\n\n# calculate error for naive model on test set\ntestNaiveMAE = mean_absolute_error(mergeData.AggregateLoad[train_size+val_size:], NaiveForecast[train_size+val_size:])\n\nprint('valNaiveMAE: ', valNaiveMAE)\nprint('testNaiveMAE: ', testNaiveMAE)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:49:01.682751Z","iopub.execute_input":"2023-10-02T00:49:01.683146Z","iopub.status.idle":"2023-10-02T00:49:01.709660Z","shell.execute_reply.started":"2023-10-02T00:49:01.683117Z","shell.execute_reply":"2023-10-02T00:49:01.707961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for plotting the train and test loss curves\ndef plot_model_loss(history):\n    plt.figure(figsize=(8,4))\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epochs')\n    plt.legend(loc='upper right')\n    plt.show()\n    return","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:49:01.711154Z","iopub.execute_input":"2023-10-02T00:49:01.712191Z","iopub.status.idle":"2023-10-02T00:49:01.718628Z","shell.execute_reply.started":"2023-10-02T00:49:01.712156Z","shell.execute_reply":"2023-10-02T00:49:01.717501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RepeatBaseline(tf.keras.Model):\n  def call(self, inputs):\n    return inputs\n\nrepeat_baseline = RepeatBaseline()\nrepeat_baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n                        metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\nmulti_val_performance['Naive'] = repeat_baseline.evaluate(mergeDataNormed.AggregateLoad[train_size:train_size+val_size], NaiveForecastNormed.AggregateLoad[train_size:train_size+val_size])\nmulti_performance['Naive'] = repeat_baseline.evaluate(mergeDataNormed.AggregateLoad[train_size+val_size:], NaiveForecastNormed.AggregateLoad[train_size+val_size:], verbose=0)\n\n# multi_val_performance['Repeat'] = repeat_baseline.evaluate(multi_window.val)\n# multi_performance['Repeat'] = repeat_baseline.evaluate(multi_window.test, verbose=0)\n# multi_window.plot(repeat_baseline)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:49:01.720034Z","iopub.execute_input":"2023-10-02T00:49:01.720541Z","iopub.status.idle":"2023-10-02T00:49:02.632101Z","shell.execute_reply.started":"2023-10-02T00:49:01.720513Z","shell.execute_reply":"2023-10-02T00:49:02.630470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_dense_model = tf.keras.Sequential([\n    # Take the last time step.\n    # Shape [batch, time, features] => [batch, 1, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n    # Shape => [batch, 1, dense_units]\n    tf.keras.layers.Dense(256, activation='relu'),\n    # Shape => [batch, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_dense_model, multi_window)\nmulti_dense_model.save('multi_dense_model.keras')\nplot_model_loss(history)\n\n# IPython.display.clear_output()\nmulti_val_performance['Dense'] = multi_dense_model.evaluate(multi_window.val)\nmulti_performance['Dense'] = multi_dense_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_dense_model)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:49:02.633535Z","iopub.execute_input":"2023-10-02T00:49:02.633919Z","iopub.status.idle":"2023-10-02T00:51:48.059475Z","shell.execute_reply.started":"2023-10-02T00:49:02.633889Z","shell.execute_reply":"2023-10-02T00:51:48.057690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN","metadata":{}},{"cell_type":"code","source":"CONV_WIDTH = 10\nmulti_conv_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n    # Shape => [batch, 1, conv_units]\n    tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),\n    # Shape => [batch, 1,  out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_conv_model, multi_window)\nmulti_conv_model.save('multi_conv_model.keras')\nplot_model_loss(history)\n# IPython.display.clear_output()\n\nmulti_val_performance['Conv'] = multi_conv_model.evaluate(multi_window.val)\nmulti_performance['Conv'] = multi_conv_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_conv_model)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:51:48.061434Z","iopub.execute_input":"2023-10-02T00:51:48.062211Z","iopub.status.idle":"2023-10-02T00:55:09.588837Z","shell.execute_reply.started":"2023-10-02T00:51:48.062171Z","shell.execute_reply":"2023-10-02T00:55:09.587102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_rnn_model = tf.keras.Sequential([\n    tf.keras.layers.SimpleRNN(256, return_sequences=False),\n    # Shape => [batch, out_steps*features].\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features].\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_rnn_model, multi_window)\nplot_model_loss(history)\nmulti_rnn_model.save('multi_rnn_model.keras')\n# IPython.display.clear_output()\n\nmulti_val_performance['RNN'] = multi_rnn_model.evaluate(multi_window.val)\nmulti_performance['RNN'] = multi_rnn_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_rnn_model)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T00:55:09.591203Z","iopub.execute_input":"2023-10-02T00:55:09.591687Z","iopub.status.idle":"2023-10-02T01:36:01.130931Z","shell.execute_reply.started":"2023-10-02T00:55:09.591651Z","shell.execute_reply":"2023-10-02T01:36:01.129371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM Model","metadata":{}},{"cell_type":"code","source":"multi_lstm_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units].\n    # Adding more `lstm_units` just overfits more quickly.\n    tf.keras.layers.LSTM(256, return_sequences=False),\n    # Shape => [batch, out_steps*features].\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features].\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nstart_time = time.time()\nhistory = compile_and_fit(multi_lstm_model, multi_window)\nprint('%s seconds' % (time.time() - start_time))\nmulti_lstm_model.save('multi_lstm_model.keras')\nplot_model_loss(history)\n# IPython.display.clear_output()\n\nmulti_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val)\nmulti_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_lstm_model)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T01:36:01.133054Z","iopub.execute_input":"2023-10-02T01:36:01.133558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN-LSTM","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.arange(len(multi_performance))\nwidth = 0.3\n\nmetric_name = 'mean_absolute_error'\nmetric_index = multi_lstm_model.metrics_names.index('mean_absolute_error')\nval_mae = [v[metric_index] for v in multi_val_performance.values()]\ntest_mae = [v[metric_index] for v in multi_performance.values()]\n\nplt.bar(x - 0.17, val_mae, width, label='Validation')\nplt.bar(x + 0.17, test_mae, width, label='Test')\nplt.xticks(ticks=x, labels=multi_performance.keys(),\n           rotation=45)\nplt.ylabel(f'MAE (average over all times and outputs)')\n_ = plt.legend()\nplt.savefig('model performance.png', format='png')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build a single record dataset from start of the test dataset\nprint(mergeDataNormed)\nprint(train_size+test_size-IN_STEPS)\nprint(mergeDataNormed.iloc[train_size+test_size-IN_STEPS:train_size+test_size])\n\nds = tf.keras.utils.timeseries_dataset_from_array(\n      data=test_data[:IN_STEPS],\n      targets=None,\n      sequence_length=IN_STEPS,\n      sequence_stride=1,\n      shuffle=False,\n      batch_size=32,)\n\ntestYhatNormed = multi_lstm_model.predict(ds)\n\n#     data=test_data[:IN_STEPS],\n# data=mergeDataNormed[train_size+test_size-IN_STEPS:train_size+test_size],","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(testYhatNormed)\nprint(testYhatNormed[0,:,-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Invert stabdardization\ntestYhat = (np.array(testYhatNormed) * np.array(train_std)) + np.array(train_mean)\n\nprint(testYhat)\nprint(testYhat[0,:,-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CNN-LSTM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot of naive, LSTM and actuals","metadata":{}},{"cell_type":"code","source":"      len_prediction=[x for x in range(len(testYhat[0,:,-1]))]\n      plt.figure(figsize=(10,5))\n      # plt.plot(len_prediction, test_data[:OUT_STEPS].AggregateLoad, marker='.', label=\"actual\")\n      plt.plot(len_prediction, mergeDataNormed.AggregateLoad[train_size+val_size:train_size+val_size+OUT_STEPS], marker='.', label=\"actual\")\n      plt.plot(len_prediction, testYhatNormed[0,:,-1], 'r', label=\"LSTM prediction\")\n      plt.plot(len_prediction, NaiveForecastNormed[train_size+val_size:train_size+val_size+OUT_STEPS].AggregateLoad, 'g', label=\"Naive prediction\")\n        \n      plt.tight_layout()\n      sns.despine(top=True)\n      plt.subplots_adjust(left=0.07)\n      plt.ylabel('KWH per half hour (Normed)', size=15)\n      plt.xlabel('Time step', size=15)\n      plt.legend(fontsize=15)\n      plt.show();\n      plt.savefig('Test First Day Normed.png', format='png')\n\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"      len_prediction=[x for x in range(len(testYhat[0,:,-1]))]\n      plt.figure(figsize=(10,5))\n      # plt.plot(len_prediction, test_data[:OUT_STEPS].AggregateLoad, marker='.', label=\"actual\")\n      plt.plot(len_prediction, mergeData.AggregateLoad[train_size+val_size:train_size+val_size+OUT_STEPS], marker='.', label=\"actual\")\n      plt.plot(len_prediction, testYhat[0,:,-1], 'r', label=\"LSTM prediction\")\n      plt.plot(len_prediction, NaiveForecast[train_size+val_size:train_size+val_size+OUT_STEPS], 'g', label=\"Naive prediction\")\n        \n      plt.tight_layout()\n      sns.despine(top=True)\n      plt.subplots_adjust(left=0.07)\n      plt.ylabel('KWH per half hour', size=15)\n      plt.xlabel('Time step', size=15)\n      plt.legend(fontsize=15)\n      plt.show()\n      plt.savefig('Test First Day.png', format='png')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Estimate financial impact of improved forecast","metadata":{}},{"cell_type":"code","source":"# Area Under Curve\nfrom scipy.integrate import simpson\nfrom numpy import trapz\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# install AutoGluon AutoML\n!pip install autogluon\nfrom autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoData = pd.read_csv('/kaggle/working/autoDataFinal.csv')\nautoData","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AutoGluon specific data preparation\n# Split the time series data into train, test, and validation datasets\ntrain_size = int(len(autoData) * 0.7)  # 70% for training\nval_size = int(len(autoData) * 0.2)   # 20% for validation\ntest_size = len(autoData) - val_size - test_size  # Remaining for testing\n\nag_train_data = autoData[:train_size]\nag_val_data = autoData[train_size:train_size+val_size]\nag_test_data = autoData[train_size+val_size:]\n\nprint('ag_train_data:\\n', ag_train_data)\nprint('ag_val_data:\\n', ag_val_data)\nprint('ag_test_data\\n', ag_test_data)\n# print(ag_train_data.info())\n\n# AutoGluon requires an ItemID Column, so adding one...\nag_train_data['item_id'] = 'LoadSum'\nag_train_data = ag_train_data.astype({\"item_id\": str})\nag_val_data['item_id'] = 'LoadSum'\nag_val_data = ag_val_data.astype({\"item_id\": str})\nag_test_data['item_id'] = 'LoadSum'\nag_test_data = ag_test_data.astype({\"item_id\": str})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a quick look at the split datasets\nprint('ag_train_data\\n', ag_train_data)\nprint('ag_val_data\\n', ag_val_data)\nprint('ag_test_data\\n', ag_test_data)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load training data in to required AutoGluon proprietary data frame\n# print(ag_train_data.info())\nag_train_data_tsdf = TimeSeriesDataFrame.from_data_frame(\n    ag_train_data,\n    id_column=\"item_id\",\n    timestamp_column=\"DateTime\"\n)\nag_train_data_tsdf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load validation data in to required AutoGluon proprietary data frame, \"_tsdf\" suffix = time series data frame\nag_val_data_tsdf = TimeSeriesDataFrame.from_data_frame(\n    ag_val_data,\n    id_column=\"item_id\",\n    timestamp_column=\"DateTime\"\n)\nag_val_data_tsdf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load test data in to required AutoGluon proprietary data frame\n# print(ag_test_data.info())\nag_test_data_tsdf = TimeSeriesDataFrame.from_data_frame(\n    ag_test_data,\n    id_column=\"item_id\",\n    timestamp_column=\"DateTime\"\n)\nag_test_data_tsdf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# at \"high_quality\" level, training takes about 45 minutes...\n# training takes about 15 minutes for DeepAR\n# training takes about 21 minutes for TemporalFusionTransformer\n# training takes about 4 minutes for PatchTST\n# training takes about 4 minutes for PatchTST","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ag_predictor = TimeSeriesPredictor(\n    prediction_length=48,\n    path=\"autogluon-london-half-hourly\",\n    target=\"AggregateLoad\",\n    eval_metric=\"MASE\",\n)\n\nag_predictor.fit(\n    ag_train_data_tsdf,\n    presets=\"medium_quality\",\n    time_limit=6000,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The test score is computed using the last\n# prediction_length=48 timesteps of each time series in test_data\nag_predictor.leaderboard(ag_val_data_tsdf, silent=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate predictions\nag_predictions = ag_predictor.predict(ag_val_data_tsdf)\nag_predictions.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_mean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# standardize the predictions so we can compare prediction errors with other models\nag_predictionsNormed = (ag_predictions[\"mean\"] - train_mean[\"AggregateLoad\"]) / train_std[\"AggregateLoad\"]\nag_predictionsNormed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot prediction results, history and actual test data values\nplt.figure(figsize=(20, 3))\n\nitem_id = \"LoadSum\"\ny_past = ag_val_data_tsdf.loc[item_id][\"AggregateLoad\"]\ny_pred = ag_predictions.loc[item_id]\ny_val = ag_test_data_tsdf.loc[item_id][\"AggregateLoad\"]\n# y_val = ag_val_data_tsdf.loc[item_id][\"AggregateLoad\"]\n\nplt.plot(y_past[-100:], label=\"Past time series values\")\nplt.plot(y_pred[\"mean\"], label=\"Mean forecast\")\nplt.plot(y_val[:48], label=\"Future time series values\")\n\nplt.fill_between(\n    y_pred.index, y_pred[\"0.1\"], y_pred[\"0.9\"], color=\"red\", alpha=0.1, label=f\"10%-90% confidence interval\"\n)\nplt.legend();\nplt.savefig('AutoML forecast.png', format='png')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# load test data in to required AutoGluon proprietary data frame\n# print(ag_test_data.info())\nag_testday1_data_tsdf = TimeSeriesDataFrame.from_data_frame(\n    ag_val_data[-IN_STEPS:],\n    id_column=\"item_id\",\n    timestamp_column=\"DateTime\"\n)\nag_testday1_data_tsdf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate predictions\nagTestDay1_predictions = ag_predictor.predict(ag_testday1_data_tsdf)\nagTestDay1_predictions.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agTestDay1_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"      len_prediction=[x for x in range(len(testYhat[0,:,-1]))]\n      plt.figure(figsize=(10,5))\n      # plt.plot(len_prediction, test_data[:OUT_STEPS].AggregateLoad, marker='.', label=\"actual\")\n      plt.plot(len_prediction, mergeData.AggregateLoad[train_size+val_size:train_size+val_size+OUT_STEPS], marker='.', label=\"actual\")\n      plt.plot(len_prediction, testYhat[0,:,-1], 'r', label=\"LSTM prediction\")\n      plt.plot(len_prediction, NaiveForecast[train_size+val_size:train_size+val_size+OUT_STEPS], 'g', label=\"Naive prediction\")\n      plt.plot(len_prediction, agTestDay1_predictions[\"mean\"], 'y', label=\"AutoML prediction\")\n        \n      plt.tight_layout()\n      sns.despine(top=True)\n      plt.subplots_adjust(left=0.07)\n      plt.ylabel('KWH per half hour', size=15)\n      plt.xlabel('Time step', size=15)\n      plt.legend(fontsize=15)\n      plt.show()\n      plt.savefig('Test First Day.png', format='png')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate error for naive model on validation set\n# valAutoMLMAE = mean_absolute_error(mergeDataNormed.AggregateLoad[train_size:train_size+val_size], ag_predictionsNormed[train_size:train_size+val_size])\n\n# calculate error for naive model on test set\ntestAutoMLMAE = mean_absolute_error(mergeDataNormed.AggregateLoad[train_size+val_size:], ag_predictionsNormed)\n\n# print('valAutoMLMAE: ', valAutoMLMAE)\nprint('testAutoMLMAE: ', testAutoMLMAE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import dill\n# save\ndill.dump_session('notebook_env.db')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}