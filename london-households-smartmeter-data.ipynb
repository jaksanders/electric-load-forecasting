{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Short-term residential load forecasting with Deep Learning\n\nLondon Households SmartMeter Data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-21T15:29:14.290314Z","iopub.execute_input":"2023-09-21T15:29:14.290690Z","iopub.status.idle":"2023-09-21T15:29:14.360653Z","shell.execute_reply.started":"2023-09-21T15:29:14.290661Z","shell.execute_reply":"2023-09-21T15:29:14.359518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nimport IPython\nimport IPython.display\nimport glob\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nrandomState = 42 # tip of the cap to Douglas Adams\n#!pip install -U pip\n#!pip install -U setuptools wheel\n\n#!pip install autogluon\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:29:14.363441Z","iopub.execute_input":"2023-09-21T15:29:14.364301Z","iopub.status.idle":"2023-09-21T15:29:14.371086Z","shell.execute_reply.started":"2023-09-21T15:29:14.364258Z","shell.execute_reply":"2023-09-21T15:29:14.370198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creat two load forecasts...\n1) half-hourly load forecast for next 24 hours\n2) peak half-hour in the next 24 hours\n\nValue...\n* For electric network operator, minimize the amount of spinning reserve\n\nData...\n* residential smart meter usage data\n* weather data\n* weather forecast data","metadata":{}},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"markdown","source":"## Load half-hourly electric usage data\n...for ~5k smart meters in London\n[SmartMeter Energy Consumption Data in London Households](https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households)","metadata":{}},{"cell_type":"code","source":"import time\n# load half-hourly electric usage data\n# takes about four minutes, need to find somerthing faster like Dask?\n# https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households\nd = pd.read_csv('/kaggle/input/small-lcl-data/LCL-June2015v2_99.csv', parse_dates=[\"DateTime\"])\n\n# Get CSV files list from a folder\npath = '/kaggle/input/smart-meters-in-london/halfhourly_dataset/halfhourly_dataset'\ncsv_files = glob.glob(path + \"/*.csv\")\n\n# Read each CSV file into DataFrame\n# This creates a list of dataframes\nstart_time = time.time()\ndf_list = (pd.read_csv(file, parse_dates=[\"tstp\"]) for file in csv_files)\nprint('%s seconds' % (time.time() - start_time))\n\n# Concatenate all DataFrames\nstart_time = time.time()\nd = pd.concat(df_list, ignore_index=True)\nprint('%s seconds' % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:29:14.372695Z","iopub.execute_input":"2023-09-21T15:29:14.373403Z","iopub.status.idle":"2023-09-21T15:34:06.419680Z","shell.execute_reply.started":"2023-09-21T15:29:14.373362Z","shell.execute_reply":"2023-09-21T15:34:06.418102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:34:06.421619Z","iopub.execute_input":"2023-09-21T15:34:06.422117Z","iopub.status.idle":"2023-09-21T15:35:52.333289Z","shell.execute_reply.started":"2023-09-21T15:34:06.422069Z","shell.execute_reply":"2023-09-21T15:35:52.331887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load hourly weather data","metadata":{}},{"cell_type":"code","source":"# load hourly weather data\n# https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households\nweatherData = pd.read_csv('/kaggle/input/smart-meters-in-london/weather_hourly_darksky.csv', parse_dates=[\"time\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:35:52.336821Z","iopub.execute_input":"2023-09-21T15:35:52.337227Z","iopub.status.idle":"2023-09-21T15:35:52.411726Z","shell.execute_reply.started":"2023-09-21T15:35:52.337183Z","shell.execute_reply":"2023-09-21T15:35:52.410396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weatherData.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:35:52.413555Z","iopub.execute_input":"2023-09-21T15:35:52.414047Z","iopub.status.idle":"2023-09-21T15:35:52.463695Z","shell.execute_reply.started":"2023-09-21T15:35:52.414011Z","shell.execute_reply":"2023-09-21T15:35:52.461941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weatherData.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:35:52.465278Z","iopub.execute_input":"2023-09-21T15:35:52.466384Z","iopub.status.idle":"2023-09-21T15:35:52.501877Z","shell.execute_reply.started":"2023-09-21T15:35:52.466343Z","shell.execute_reply":"2023-09-21T15:35:52.500707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data pre-processing and cleaning","metadata":{}},{"cell_type":"markdown","source":"## Wetaher data: convert test attributes to string datatype","metadata":{}},{"cell_type":"code","source":"weatherData = weatherData.astype({'precipType':'string', 'icon':'string', 'summary':'string'})\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:35:52.503042Z","iopub.execute_input":"2023-09-21T15:35:52.503380Z","iopub.status.idle":"2023-09-21T15:35:52.515559Z","shell.execute_reply.started":"2023-09-21T15:35:52.503350Z","shell.execute_reply":"2023-09-21T15:35:52.514205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas_profiling import ProfileReport\n\nprofile = ProfileReport(weatherData, tsmode=True, sortby=\"time\")\nprofile.to_file('weatherData profile_report.html')\n# profile\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:35:52.517466Z","iopub.execute_input":"2023-09-21T15:35:52.517980Z","iopub.status.idle":"2023-09-21T15:36:57.816642Z","shell.execute_reply.started":"2023-09-21T15:35:52.517934Z","shell.execute_reply":"2023-09-21T15:36:57.815250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Identify and remove weather records not exactly on the hour","metadata":{}},{"cell_type":"code","source":"# inspect and remove records not exactly on the hour\noffRecs = weatherData.query(\"time.dt.minute != 0 or time.dt.second != 0\")\nprint('Records not exactly on the half-hour:\\n ', offRecs)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:36:57.818710Z","iopub.execute_input":"2023-09-21T15:36:57.819294Z","iopub.status.idle":"2023-09-21T15:36:57.841171Z","shell.execute_reply.started":"2023-09-21T15:36:57.819246Z","shell.execute_reply":"2023-09-21T15:36:57.839602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Upsample weather data to match half-houly sampling rate of load data","metadata":{}},{"cell_type":"code","source":"# select weather data features of interest\nweatherUpsample = weatherData[['time','temperature', 'dewPoint', 'pressure', 'humidity']].copy()\nweatherUpsample = weatherUpsample.sort_values(by=['time'])\nprint(weatherUpsample.info())\nprint(weatherUpsample.describe())\nprint(weatherUpsample)\n\nweatherUpsample = weatherUpsample.set_index('time')\nweatherUpsample.index.rename('time', inplace=True)\n\nstart_time = time.time()\n\nweatherUpsample = weatherUpsample.resample('30Min').mean()\n\n# upsample \nweatherUpsample['temperature'] = weatherUpsample['temperature'].interpolate()\nweatherUpsample['dewPoint'] = weatherUpsample['dewPoint'].interpolate()\nweatherUpsample['pressure'] = weatherUpsample['pressure'].interpolate()\nweatherUpsample['humidity'] = weatherUpsample['humidity'].interpolate()\n\nprint('%s seconds' % (time.time() - start_time))\n\nweatherUpsample = weatherUpsample.reset_index(names='DateTime')\nprint(weatherUpsample.info())\nprint(weatherUpsample.describe())\nprint(weatherUpsample)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:36:57.842428Z","iopub.execute_input":"2023-09-21T15:36:57.843144Z","iopub.status.idle":"2023-09-21T15:36:57.966826Z","shell.execute_reply.started":"2023-09-21T15:36:57.843110Z","shell.execute_reply":"2023-09-21T15:36:57.965664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:36:57.968190Z","iopub.execute_input":"2023-09-21T15:36:57.968946Z","iopub.status.idle":"2023-09-21T15:36:57.983652Z","shell.execute_reply.started":"2023-09-21T15:36:57.968912Z","shell.execute_reply":"2023-09-21T15:36:57.982126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# utility function to nicely format variable names and memory they are consuming\nimport sys\ndef sizeof_fmt(num, suffix='B'):\n    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:36:57.985823Z","iopub.execute_input":"2023-09-21T15:36:57.986241Z","iopub.status.idle":"2023-09-21T15:36:57.999094Z","shell.execute_reply.started":"2023-09-21T15:36:57.986207Z","shell.execute_reply":"2023-09-21T15:36:57.997409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert smart meter usage datatype to float","metadata":{}},{"cell_type":"code","source":"# ~1 minute\nstart_time = time.time()\nd.iloc[:, 2] = pd.to_numeric(d.iloc[:, 2], errors='coerce')\nprint('%s seconds' % (time.time() - start_time))\n\n# rename usage column for easier reference\nd.rename(columns={d.columns[2]: 'KWHperHH'}, inplace=True)\nd.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:36:58.003612Z","iopub.execute_input":"2023-09-21T15:36:58.004000Z","iopub.status.idle":"2023-09-21T15:38:59.791312Z","shell.execute_reply.started":"2023-09-21T15:36:58.003969Z","shell.execute_reply":"2023-09-21T15:38:59.789205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set timestamp as the index\nstart_time = time.time()\nd.set_index('tstp')\nprint('%s seconds' % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:38:59.793469Z","iopub.execute_input":"2023-09-21T15:38:59.794331Z","iopub.status.idle":"2023-09-21T15:39:05.906842Z","shell.execute_reply.started":"2023-09-21T15:38:59.794282Z","shell.execute_reply":"2023-09-21T15:39:05.905866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Identify and handle duplicates in the smart meter data","metadata":{}},{"cell_type":"code","source":"# about 1.5 minutes\nstart_time = time.time()\ndupes = d[d.duplicated()]\nprint('dupes', dupes)\nprint('dupes.index', dupes.index)\nd.drop(index=dupes.index, inplace=True)\nprint('%s seconds' % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:39:05.908094Z","iopub.execute_input":"2023-09-21T15:39:05.908463Z","iopub.status.idle":"2023-09-21T15:40:40.284099Z","shell.execute_reply.started":"2023-09-21T15:39:05.908428Z","shell.execute_reply":"2023-09-21T15:40:40.282734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set index for the usage data to the timestamp column.  Is this necessary?  Can't remember why\nstart_time = time.time()\nd.set_index('tstp')\nd.info()\nprint('%s seconds' % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:40:40.285533Z","iopub.execute_input":"2023-09-21T15:40:40.285915Z","iopub.status.idle":"2023-09-21T15:40:44.568756Z","shell.execute_reply.started":"2023-09-21T15:40:40.285879Z","shell.execute_reply":"2023-09-21T15:40:44.567205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check what is gobbling RAM\nfor name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(\n                          locals().items())), key= lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:40:44.571147Z","iopub.execute_input":"2023-09-21T15:40:44.571721Z","iopub.status.idle":"2023-09-21T15:41:35.266459Z","shell.execute_reply.started":"2023-09-21T15:40:44.571652Z","shell.execute_reply":"2023-09-21T15:41:35.265186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize smart meter dataset to analyze for quality, completenes and othe insights","metadata":{}},{"cell_type":"code","source":"import seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-09-21T15:41:35.267851Z","iopub.execute_input":"2023-09-21T15:41:35.268197Z","iopub.status.idle":"2023-09-21T15:41:35.273889Z","shell.execute_reply.started":"2023-09-21T15:41:35.268166Z","shell.execute_reply":"2023-09-21T15:41:35.272323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## grab a random sample of 2% of meters for visualization and analysis","metadata":{}},{"cell_type":"code","source":"rng = np.random.default_rng(randomState)\n# random_state = np.random.RandomState(randomState)\nsampleMeters = rng.choice(d.LCLid.unique(), size=int(len(d.LCLid.unique())*0.02), replace=False)\n# sampleMeters = np.random.choice(d.LCLid.unique(), size=int(len(d.LCLid.unique())*0.02), replace=False, random_state=random_state)\nprint('sampleMeters:\\n', sampleMeters)\nsample = d[d['LCLid'].isin(sampleMeters)]\nprint('sample:\\n', sample)\nprint(sample.describe())\n# print(sample.info())","metadata":{"execution":{"iopub.status.busy":"2023-09-21T16:10:59.220287Z","iopub.execute_input":"2023-09-21T16:10:59.220723Z","iopub.status.idle":"2023-09-21T16:11:42.245391Z","shell.execute_reply.started":"2023-09-21T16:10:59.220691Z","shell.execute_reply":"2023-09-21T16:11:42.244262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Heatmap to visualize meter read coverage and completeness","metadata":{}},{"cell_type":"code","source":"# visualize meter read coverage and completeness\n# using a random sample of 2% of meters\nplt.subplots(figsize=(20,15))\npivot_table = pd.pivot_table(sample, columns='tstp', index='LCLid', values='KWHperHH')\nsns.heatmap(pivot_table)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T16:13:54.339450Z","iopub.execute_input":"2023-09-21T16:13:54.339941Z","iopub.status.idle":"2023-09-21T16:14:10.670441Z","shell.execute_reply.started":"2023-09-21T16:13:54.339900Z","shell.execute_reply":"2023-09-21T16:14:10.669115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations from Heatmap...\n* several houses start producing load part-way through the period\n    - eg MAC004221, MAC004248\n    \n    \n* several houses stop producing part-way through the period\n    - eg MAC004226, MAC004257\n    \n\n* most houses have at least one \"gap\" in their data (visible as white lines)\n\n\n* several houses stand out as having significantly higher average load than others\n    - eg MAC004225, MAC004249","metadata":{}},{"cell_type":"markdown","source":"## identify and remove smart meter readings not exactly on the half-hour","metadata":{}},{"cell_type":"code","source":"# identify and remove records not exactly on the half-hour\n\nstart_time = time.time()\n\noffRecs = d.query(\"tstp.dt.minute not in (0,30) or tstp.dt.second != 0\")\n# aggLoad[\"DateTime\"].dt.hour > 30\nprint('Records not exactly on the half-hour:\\n ', offRecs)\nprint(offRecs.info())\n\n# delete records not exactly on the half-hour\nd.drop(offRecs.index, inplace=True)\n\nprint('%s seconds' % (time.time() - start_time))\n\noffRecs = d.query(\"tstp.dt.minute not in (0,30) or tstp.dt.second != 0\")\nprint('Records not exactly on the half-hour: ', offRecs)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T16:15:54.113868Z","iopub.execute_input":"2023-09-21T16:15:54.114919Z","iopub.status.idle":"2023-09-21T16:17:16.194479Z","shell.execute_reply.started":"2023-09-21T16:15:54.114874Z","shell.execute_reply":"2023-09-21T16:17:16.193385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:59:02.028920Z","iopub.status.idle":"2023-09-21T14:59:02.029350Z","shell.execute_reply.started":"2023-09-21T14:59:02.029145Z","shell.execute_reply":"2023-09-21T14:59:02.029164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check what is gobbling RAM\nfor name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(\n                          locals().items())), key= lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:59:02.030553Z","iopub.status.idle":"2023-09-21T14:59:02.030969Z","shell.execute_reply.started":"2023-09-21T14:59:02.030761Z","shell.execute_reply":"2023-09-21T14:59:02.030779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fill gaps in the smart meter data using interpolation\n\nOur heatmap above shows lots of gaps (small white vertical lines), and we'll fill those gaps using interpolation","metadata":{}},{"cell_type":"markdown","source":"### First step of filling these gaps is to create NaN records where records are missing\n\nThen we can fill these gas with interpolation","metadata":{}},{"cell_type":"code","source":"# First step of interpolation is to create NaN records where records are missing\n# about 2 minutes\nd.sort_values(by=['tstp'], inplace=True)\nd.set_index('tstp', inplace=True)\nd.index.rename('tstp', inplace=True)\n\nstart_time = time.time()\n# resample to create NaN records where records are missing\nd = d.groupby('LCLid')\\\n                .resample('30Min')\\\n                .mean()\n\n# fill the gaps with interpolation\nd['KWHperHH'] = d['KWHperHH'].interpolate()\nd.reset_index(inplace=True)\n\nprint('%s seconds' % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T16:21:15.184539Z","iopub.execute_input":"2023-09-21T16:21:15.185037Z","iopub.status.idle":"2023-09-21T16:24:45.037827Z","shell.execute_reply.started":"2023-09-21T16:21:15.184996Z","shell.execute_reply":"2023-09-21T16:24:45.036580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check the meter data heatmap to see if gaps have been filled","metadata":{}},{"cell_type":"code","source":"# visualize after interpolating missing values\nd.info()\nsample = d[d['LCLid'].isin(sampleMeters)]\npivot_table = pd.pivot_table(sample, columns='tstp', index='LCLid', values='KWHperHH')\nplt.subplots(figsize=(20,15))\n\nsns.heatmap(pivot_table)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T16:28:51.379888Z","iopub.execute_input":"2023-09-21T16:28:51.380353Z","iopub.status.idle":"2023-09-21T16:29:19.312314Z","shell.execute_reply.started":"2023-09-21T16:28:51.380317Z","shell.execute_reply":"2023-09-21T16:29:19.309306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize zeros in the dataset using heatmap\n\nI'm always curious to understand zeros in a dataset, and whether they are legitimate zero values, or indicate a data quality problem.","metadata":{}},{"cell_type":"code","source":"# visualize zeros in the dataset\nstart_time = time.time()\nsample = d[d['LCLid'].isin(sampleMeters)]\nsample['ZeroKWHperHH'] = sample['KWHperHH'] == 0\npivot_table = pd.pivot_table(sample, columns='tstp', index='LCLid', values='ZeroKWHperHH')\nprint('%s seconds' % (time.time() - start_time))\nplt.subplots(figsize=(20,15))\n\nsns.heatmap(pivot_table)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T16:33:46.081660Z","iopub.execute_input":"2023-09-21T16:33:46.082102Z","iopub.status.idle":"2023-09-21T16:34:13.222859Z","shell.execute_reply.started":"2023-09-21T16:33:46.082064Z","shell.execute_reply":"2023-09-21T16:34:13.221670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Obervation\n\nThere are a handful of households that account all the zero value meter reads: MAC004233, MAC004226, MAC004267","metadata":{}},{"cell_type":"code","source":"# investigate the meters with zero reads\nMAC005127 = sample.query(\"LCLid == 'MAC005127'\")\n\nfig, ax = plt.subplots(4,figsize=(20,9))\n\n# plot whole ~2 years\nax[0].plot(MAC005127.tstp, MAC005127.KWHperHH)\nax[0].plot(MAC005127.tstp, MAC005127.ZeroKWHperHH)\nax[0].set(ylabel='KWH/hh',\n       title='Load from one Household MAC004233 with lots of zero values')\nplt.tick_params(rotation=45)\nax[0].grid()\n\n# zoom in\nax[1].plot(MAC005127.tstp[11000:15000], MAC005127.KWHperHH[11000:15000])\nax[1].plot(MAC005127.tstp[11000:15000], MAC005127.ZeroKWHperHH[11000:15000])\nax[1].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[1].grid()\n\n# zoom in more...\nax[2].plot(MAC005127.tstp[13000:13500], MAC005127.KWHperHH[13000:13500])\nax[2].plot(MAC005127.tstp[13000:13500], MAC005127.ZeroKWHperHH[13000:13500])\nax[2].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[2].grid()\n\n# zoom in to a different part of the series...\nax[3].plot(MAC005127.tstp[25000:25500], MAC005127.KWHperHH[25000:25500])\nax[3].plot(MAC005127.tstp[25000:25500], MAC005127.ZeroKWHperHH[25000:25500])\nax[3].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[3].grid()\n\nfig.savefig(\"MAC005127.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T16:36:50.604463Z","iopub.execute_input":"2023-09-21T16:36:50.605017Z","iopub.status.idle":"2023-09-21T16:36:52.211411Z","shell.execute_reply.started":"2023-09-21T16:36:50.604970Z","shell.execute_reply":"2023-09-21T16:36:52.210100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation:\nThe zeros for MAC004233 seem legit - leaving them in","metadata":{}},{"cell_type":"code","source":"# investigate the meters with zero reads\nMAC002304 = sample.query(\"LCLid == 'MAC002304'\")\nfig, ax = plt.subplots(4,figsize=(20,9))\n\n# plot whole ~2 years\nax[0].plot(MAC002304.tstp, MAC002304.KWHperHH)\nax[0].plot(MAC002304.tstp, MAC002304.ZeroKWHperHH)\nax[0].set(ylabel='KWH/hh',\n       title='Load from one Household MAC002304 with lots of zero values')\nplt.tick_params(rotation=45)\nax[0].grid()\n\n# zoom in\nax[1].plot(MAC002304.tstp[17000:21000], MAC002304.KWHperHH[17000:21000])\nax[1].plot(MAC002304.tstp[17000:21000], MAC002304.ZeroKWHperHH[17000:21000])\nax[1].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[1].grid()\n\n# zoom in more...\nax[2].plot(MAC002304.tstp[19300:19800], MAC002304.KWHperHH[19300:19800])\nax[2].plot(MAC002304.tstp[19300:19800], MAC002304.ZeroKWHperHH[19300:19800])\nax[2].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[2].grid()\n\n# zoom in to a different part of the series...\nax[3].plot(MAC002304.tstp[25000:25500], MAC002304.KWHperHH[25000:25500])\nax[3].plot(MAC002304.tstp[25000:25500], MAC002304.ZeroKWHperHH[25000:25500])\nax[3].set(xlabel='time (s)', ylabel='KWH/hh')\nplt.tick_params(rotation=45)\nax[3].grid()\n\nfig.savefig(\"MAC002304.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T16:36:52.213458Z","iopub.execute_input":"2023-09-21T16:36:52.213835Z","iopub.status.idle":"2023-09-21T16:36:53.776013Z","shell.execute_reply.started":"2023-09-21T16:36:52.213801Z","shell.execute_reply":"2023-09-21T16:36:53.775009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observation\n\nThe zeros for MAC004233 seem legit - leaving them in\n","metadata":{}},{"cell_type":"code","source":"# visualize and handle outliers\n\n# minumum and maximum timestamp for each house\nprint(d.groupby('LCLid').max().sort_values('tstp'))\nprint(d.groupby('LCLid').min().sort_values('tstp'))\nprint(d.groupby('LCLid').count().sort_values('tstp'))\n\nprint(d.groupby('LCLid').agg(['min', 'max', 'count']))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T16:36:53.777316Z","iopub.execute_input":"2023-09-21T16:36:53.778309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# which house has the highest peak load?\n\n# which house has the highest total aggregate load?\n\n# how variable / predictable is the timing of the peak load\n\n# how accurate is the next 24 hours forecast profile overall?\n\n# how accurate is the peak load forecast in next 24 hours?\n\n# normalize and standardize\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract one smartmeter for plotting\nsample = d.query(\"LCLid == 'MAC004233'\")\nsample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize load profile for one household meter\nfig, ax = plt.subplots()\nax.plot(sample.iloc[100:4500,1], sample.iloc[100:4500,2])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Load from one Household, June-September 2012')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Load from one Household, June-September 2012.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set index for the sample\nsample.set_index('tstp')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA: Visualize daily average load for each meter and all meters...","metadata":{}},{"cell_type":"code","source":"# calculate average daily load profile for all meters...\n# abut 1.5 minutes\n\nstart_time = time.time()\navgLoadProfile = pd.DataFrame(d.groupby([d['tstp'].dt.hour, d['tstp'].dt.minute]).KWHperHH.mean())\navgLoadProfile = avgLoadProfile.reset_index(names=['hour', 'minute'])\navgLoadProfile['labels'] = pd.to_datetime(avgLoadProfile['hour'].astype(str) + ':' + avgLoadProfile['minute'].astype(str), format='%H:%M').dt.time\nprint('%s seconds' % (time.time() - start_time))\n\n# print(avgLoadProfile.info())\n# print(avgLoadProfile)\n\nfig, ax = plt.subplots(figsize=(10,7))\n\nax.set_xticks(avgLoadProfile.index, avgLoadProfile.labels)\n\nax.set(xlabel='time (HH:MI)', ylabel='KWH/hh',\n       title='Average Household 24 hour load profile')\n\n# calculate average daily load for each meter...\nstart_time = time.time()\navgLoadProfileEachMeter = pd.DataFrame(d.groupby(['LCLid', d['tstp'].dt.hour, d['tstp'].dt.minute]).agg({'KWHperHH': 'mean'}))\navgLoadProfileEachMeter = avgLoadProfileEachMeter.reset_index(names=['LCLid', 'hour', 'minute'])\nprint('%s seconds' % (time.time() - start_time))\n# print(avgLoadProfileEachMeter.info())\n# print(avgLoadProfileEachMeter)\n\n# plot every sample meter\nfor meter in sampleMeters:\n    # print(meter)\n    ax.plot(avgLoadProfileEachMeter.loc[avgLoadProfileEachMeter['LCLid'] == meter].index % 48, \n            avgLoadProfileEachMeter.loc[avgLoadProfileEachMeter['LCLid'] == meter].KWHperHH,\n           color='grey')\n\n# plot the average\nax.plot(avgLoadProfile.index, avgLoadProfile.KWHperHH, linewidth=5)\n\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Avg 24hr Load Profile every meter.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the sum of all loads for each timestamp using `groupby()` and `agg()`\nstart_time = time.time()\n# aggLoad = d.groupby('tstp')['KWHperHH'].agg('sum')\naggLoad = pd.DataFrame(d.groupby('tstp')['KWHperHH'].agg('sum'))\naggLoad.reset_index(inplace=True)\naggLoad.columns = ['tstp', 'AggregateLoad']\nprint('%s seconds' % (time.time() - start_time))\n\nprint(aggLoad)\nprint(aggLoad.describe())\nprint(aggLoad.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggLoad.sort_values(by=['tstp'], inplace=True)\naggLoad.set_index('tstp', inplace=True)\naggLoad.index.rename('DateTimeIndex', inplace=True)\naggLoad.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggLoad['DateTime'] = aggLoad.index\naggLoad.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inspect and fix records with zero load\n# start with the aggregated records with zero load\nAggZeros = aggLoad.query(\"AggregateLoad == 0\")\nAggZeros\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation: Some of the timestamps are not exactly on the half-hour\nQuestion: How many of the timestamps are not exactly on the half-hour?","metadata":{}},{"cell_type":"code","source":"# inspect and fix records not exactly on the half-hour\noffRecs = aggLoad.query(\"DateTime.dt.minute not in (0,30) or DateTime.dt.second != 0\")\n# aggLoad[\"DateTime\"].dt.hour > 30\nprint('Records not exactly on the half-hour: ', offRecs)\nprint(offRecs.info())\n\n# delete records not exactly on the half-hour\naggLoad = aggLoad.drop(offRecs.index)\n\noffRecs = aggLoad.query(\"DateTime.dt.minute not in (0,30) or DateTime.dt.second != 0\")\nprint('Records not exactly on the half-hour: ', offRecs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for missing records in the aggregate load time series\n# create reference time series\nminTimestamp = aggLoad.index.min()\nmaxTimestamp = aggLoad.index.max()\n\nprint('minTimestamp: ', minTimestamp)\nprint('maxTimestamp: ', maxTimestamp)\n\ndate_range = pd.date_range(minTimestamp, maxTimestamp, freq='30Min')\nreference_df = pd.DataFrame(np.random.randint(1, 20, (date_range.shape[0], 1)))\nreference_df.index = date_range  # set index\n\nprint('reference index length: ', reference_df.shape)\nprint('aggLoad index length: ', aggLoad.shape)\n\nprint('reference_df: ', reference_df)\nprint('aggLoad: ', aggLoad)\n\nprint('reference index: ', reference_df.index)\nprint('aggLoad index: ', aggLoad.index)\n\n# check for missing datetimeindex values based on reference index (with all values)\nmissing_dates = reference_df.index[~reference_df.index.isin(aggLoad.index)]\n\nprint('missing_dates: ', missing_dates)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the regularity of the observations (time between observations)\n# print(pd.infer_freq(train_data.DateTime))\naggLoad.index.to_series().diff().value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate moving average and stddev\nwindow_size = int(len(aggLoad.AggregateLoad) / 10)\nprint(window_size)\n\naggLoadMovingStdev = aggLoad.AggregateLoad.rolling(window_size).std()\naggLoadMovingStdev.columns = ['MovingStdev']\n# aggLoadMovingStdev.columns.values[0] = 'MovingStdev'\n\naggLoadMovingAvg = aggLoad.AggregateLoad.rolling(window_size).mean()\naggLoadMovingAvg.columns = ['MovingAvg']\n\nprint('aggLoadMovingStdev:\\n', aggLoadMovingStdev)\nprint(aggLoadMovingStdev.info())\nprint('aggLoadMovingAvg:\\n', aggLoadMovingAvg)\nprint(aggLoadMovingAvg.info())\n\n# aggLoad['MovingStdev'] = aggLoad.AggregateLoad.rolling(window_size).std()\n# aggLoad['MovingAvg'] = aggLoad.AggregateLoad.rolling(window_size).mean()\n\n# print('aggLoad.MovingStdev:\\n', aggLoad.MovingStdev)\n# print('aggLoad.MovingAvg:\\n', aggLoad.MovingAvg)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(aggLoad)\n\nfig, ax = plt.subplots(figsize=(20,7))\nax.plot(aggLoad.DateTime, aggLoad.AggregateLoad)\n# ax.plot(aggLoad.DateTime, aggLoad.MovingAvg, linewidth=3)\n# ax.plot(aggLoad.DateTime, aggLoad.MovingStdev, linewidth=3)\nax.plot(aggLoad.DateTime, aggLoadMovingAvg, linewidth=3)\nax.plot(aggLoad.DateTime, aggLoadMovingStdev, linewidth=3)\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load 2012-2014')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Aggregate Household load 2012-2014.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aggLoad.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,7))\nax.plot(aggLoad.DateTime[10000:15000], aggLoad.AggregateLoad[10000:15000])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load June-August 2012')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"Aggregate Household load June-August 2012.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,7))\nax.plot(aggLoad.DateTime[12000:13000], aggLoad.AggregateLoad[12000:13000])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"test.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,7))\nax.plot(aggLoad.DateTime[12500:12600], aggLoad.AggregateLoad[12500:12600])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load ~two days')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"test.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.plot(aggLoad.DateTime[12500:12550], aggLoad.AggregateLoad[12500:12550])\n\nax.set(xlabel='time (s)', ylabel='KWH/hh',\n       title='Aggregate Household load (one day)')\nplt.tick_params(rotation=45)\nax.grid()\n\nfig.savefig(\"test.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Join load data and weather data\nmergeData = pd.merge(aggLoad, weatherUpsample, on='DateTime', copy=False)\nprint(mergeData.info())\nmergeData\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move first column to the Last\n# df = pd.DataFrame(mergeData)\ndf = mergeData\ntemp_cols=df.columns.tolist()\nnew_cols=temp_cols[1:] + temp_cols[0:1]\nmergeData=df[new_cols]\nprint(mergeData)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas_profiling import ProfileReport\n\nprofile = ProfileReport(mergeData, tsmode=True, sortby=\"DateTime\")\nprofile.to_file('mergeData profile_report.html')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove the dateTime feature from the dataset\nautoData = mergeData.copy() # keep a copy for the autogluon AutoML\nmergeData = mergeData.drop(columns=['DateTime'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the time series data into train, test, and validation datasets\ntrain_size = int(len(mergeData) * 0.7)  # 70% for training\nval_size = int(len(mergeData) * 0.2)   # 20% for validation\ntest_size = len(mergeData) - val_size - train_size  # Remaining for testing\n\ntrain_data = mergeData[:train_size].copy()\nval_data = mergeData[train_size:train_size+val_size].copy()\ntest_data = mergeData[train_size+val_size:].copy()\n\nprint('train_data.head()', train_data.head())\nprint('val_data.head()', val_data.head())\nprint('test_data.head()', test_data.head())\nprint(train_data.info())\n\nnum_features = mergeData.shape[1]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction_plot(testY, test_predict):\n      len_prediction=[x for x in range(len(testY))]\n      plt.figure(figsize=(20,5))\n      plt.plot(len_prediction, testY, marker='.', label=\"actual\")\n      plt.plot(len_prediction, test_predict, 'r', label=\"prediction\")\n      plt.tight_layout()\n      sns.despine(top=True)\n      plt.subplots_adjust(left=0.07)\n      plt.ylabel('KWH per half hour', size=15)\n      plt.xlabel('Time step', size=15)\n      plt.legend(fontsize=15)\n      plt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize the data\ntrain_mean = train_data.mean()\ntrain_std = train_data.std()\n\ntrain_data = (train_data - train_mean) / train_std\nval_data = (val_data - train_mean) / train_std\ntest_data = (test_data - train_mean) / train_std\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize distribution of the features\ndf_std = (mergeData - train_mean) / train_std\ndf_std = df_std.melt(var_name='Column', value_name='Normalized')\nplt.figure(figsize=(12, 6))\nax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n_ = ax.set_xticklabels(mergeData.keys(), rotation=90)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WindowGenerator():\n    # https://www.tensorflow.org/tutorials/structured_data/time_series#1_indexes_and_offsets\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_data, val_df=val_data, test_df=test_data,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_window(self, features):\n    # https://www.tensorflow.org/tutorials/structured_data/time_series#2_split\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nWindowGenerator.split_window = split_window\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot(self, model=None, plot_col='AggregateLoad', max_subplots=3):\n    # https://www.tensorflow.org/tutorials/structured_data/time_series#3_plot\n  inputs, labels = self.example\n  plt.figure(figsize=(12, 8))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(max_n, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    if model is not None:\n      predictions = model(inputs)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', edgecolors='k', label='Predictions',\n                  c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [h]')\n\nWindowGenerator.plot = plot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_dataset(self, data):\n    # https://www.tensorflow.org/tutorials/structured_data/time_series#4_create_tfdatadatasets\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.utils.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=True,\n      seed=randomState,\n      batch_size=32,)\n\n  ds = ds.map(self.split_window)\n\n  return ds\n\nWindowGenerator.make_dataset = make_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.tensorflow.org/tutorials/structured_data/time_series#4_create_tfdatadatasets\n@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  result = getattr(self, '_example', None)\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    result = next(iter(self.train))\n    # And cache it for next time\n    self._example = result\n  return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare data for one-shot multi-step\nOUT_STEPS = 48 # 24 hour forecast\nIN_STEPS = 336 # look back 1 week\nmulti_window = WindowGenerator(input_width=IN_STEPS,\n                               label_width=OUT_STEPS,\n                               shift=OUT_STEPS)\n\nmulti_window.plot()\nmulti_window","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use a naive persistence model as baseline to compare more sophisticated models\nUse a 1 week persistence\n\nGeorgios Tziolis, Chrysovalantis Spanias, Maria Theodoride, Spyros Theocharides, Javier Lopez-Lorente, Andreas Livera, George Makrides, George E. Georghiou,\n\nShort-term electric net load forecasting for solar-integrated distribution systems based on Bayesian neural networks and statistical post-processing,\n\nEnergy,\nVolume 271,\n2023,\n127018,\nISSN 0360-5442,\n\nhttps://doi.org/10.1016/j.energy.2023.127018.","metadata":{}},{"cell_type":"code","source":"# capture performnce of models\nmulti_val_performance = {}\nmulti_performance = {}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.tensorflow.org/tutorials/structured_data/time_series#linear_model\nMAX_EPOCHS = 50\n\ndef compile_and_fit(model, window, patience=2):\n  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\n  history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n  return history\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Naive 1 week persistence model\nNaiveForecast = mergeData.AggregateLoad.copy()\n\nOneWeekNPeriods = 48 * 7\n\nNaiveForecast[:OneWeekNPeriods] = np.nan\n\nfor i in range(OneWeekNPeriods, len(mergeData.AggregateLoad)):\n    NaiveForecast[i] = mergeData.AggregateLoad[i - OneWeekNPeriods]\n\nprint(NaiveForecast.info())\nprint(NaiveForecast.describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize naive forecast\nprediction_plot(mergeData.AggregateLoad, NaiveForecast)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize naive forecast and actuals for first day of the test dataset\nprediction_plot(mergeData.AggregateLoad[train_size+test_size:train_size+test_size+OUT_STEPS], NaiveForecast[train_size+test_size:train_size+test_size+OUT_STEPS])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# normalize the naive \n# (NaiveForecast - train_mean) / train_std\nNaiveForecastNormed = NaiveForecast.transform(lambda x: (x - train_mean) / train_std)\nNaiveForecastNormed.describe()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize naive forecast and actuals for first day of the test dataset\nmergeDataNormed = (mergeData - train_mean) / train_std\nprediction_plot(mergeDataNormed.AggregateLoad[train_size+test_size:train_size+test_size+OUT_STEPS], NaiveForecastNormed.AggregateLoad[train_size+test_size:train_size+test_size+OUT_STEPS])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate error for naive model\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\nprint('Naive Root Mean Squared Error(RMSE): %.2f; Naive Mean Absolute Error(MAE) : %.2f; Naive Mean Absolute Percantage Error(MAPE) : %.2f '\n      % (np.sqrt(mean_squared_error(mergeData.AggregateLoad[OneWeekNPeriods:], NaiveForecast[OneWeekNPeriods:])),\n         mean_absolute_error(mergeData.AggregateLoad[OneWeekNPeriods:], NaiveForecast[OneWeekNPeriods:]),\n         mean_absolute_percentage_error(mergeData.AggregateLoad[OneWeekNPeriods:], NaiveForecast[OneWeekNPeriods:])))\n\n# calculate error for naive model on validation set\nvalNaiveMAE = mean_absolute_error(mergeData.AggregateLoad[train_size:train_size+val_size], NaiveForecast[train_size:train_size+val_size])\n\n# calculate error for naive model on test set\ntestNaiveMAE = mean_absolute_error(mergeData.AggregateLoad[train_size+val_size:], NaiveForecast[train_size+val_size:])\n\nprint('valNaiveMAE: ', valNaiveMAE)\nprint('testNaiveMAE: ', testNaiveMAE)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.tensorflow.org/tutorials/structured_data/time_series#single-shot_models\nmulti_linear_model = tf.keras.Sequential([\n    # Take the last time-step.\n    # Shape [batch, time, features] => [batch, 1, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n    # Shape => [batch, 1, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_linear_model, multi_window)\n\n# IPython.display.clear_output()\nmulti_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val)\nmulti_performance['Linear'] = multi_linear_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_linear_model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_dense_model = tf.keras.Sequential([\n    # Take the last time step.\n    # Shape [batch, time, features] => [batch, 1, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n    # Shape => [batch, 1, dense_units]\n    tf.keras.layers.Dense(512, activation='relu'),\n    # Shape => [batch, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_dense_model, multi_window)\n\n# IPython.display.clear_output()\nmulti_val_performance['Dense'] = multi_dense_model.evaluate(multi_window.val)\nmulti_performance['Dense'] = multi_dense_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_dense_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONV_WIDTH = 10\nmulti_conv_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n    # Shape => [batch, 1, conv_units]\n    tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),\n    # Shape => [batch, 1,  out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_conv_model, multi_window)\n\nIPython.display.clear_output()\n\nmulti_val_performance['Conv'] = multi_conv_model.evaluate(multi_window.val)\nmulti_performance['Conv'] = multi_conv_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_conv_model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_rnn_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units].\n    # Adding more `lstm_units` just overfits more quickly.\n    tf.keras.layers.SimpleRNN(32, return_sequences=False),\n    # Shape => [batch, out_steps*features].\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features].\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_rnn_model, multi_window)\n\n# IPython.display.clear_output()\n\nmulti_val_performance['RNN'] = multi_rnn_model.evaluate(multi_window.val)\nmulti_performance['RNN'] = multi_rnn_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_rnn_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_lstm_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units].\n    # Adding more `lstm_units` just overfits more quickly.\n    tf.keras.layers.LSTM(32, return_sequences=False),\n    # Shape => [batch, out_steps*features].\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features].\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_lstm_model, multi_window)\n\n# IPython.display.clear_output()\n\nmulti_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val)\nmulti_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_lstm_model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build a single record dataset from start of the test dataset\n\nds = tf.keras.utils.timeseries_dataset_from_array(\n      data=test_data[:IN_STEPS],\n      targets=None,\n      sequence_length=IN_STEPS,\n      sequence_stride=1,\n      shuffle=False,\n      batch_size=32,)\n\ntestYhat = multi_lstm_model.predict(ds)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(testYhat)\nprint(testYhat[0,:,-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.arange(len(multi_performance))\nwidth = 0.3\n\nmetric_name = 'mean_absolute_error'\nmetric_index = multi_lstm_model.metrics_names.index('mean_absolute_error')\nval_mae = [v[metric_index] for v in multi_val_performance.values()]\ntest_mae = [v[metric_index] for v in multi_performance.values()]\n\nplt.bar(x - 0.17, val_mae, width, label='Validation')\nplt.bar(x + 0.17, test_mae, width, label='Test')\nplt.xticks(ticks=x, labels=multi_performance.keys(),\n           rotation=45)\nplt.ylabel(f'MAE (average over all times and outputs)')\n_ = plt.legend()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for plotting the train and test loss curves\ndef model_loss(history):\n    plt.figure(figsize=(8,4))\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Test Loss')\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epochs')\n    plt.legend(loc='upper right')\n    plt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CNN-LSTM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot of naive, LSTM and actuals","metadata":{}},{"cell_type":"code","source":"      len_prediction=[x for x in range(len(testYhat[0,:,-1]))]\n      plt.figure(figsize=(10,5))\n      plt.plot(len_prediction, test_data[:OUT_STEPS].AggregateLoad, marker='.', label=\"actual\")\n      plt.plot(len_prediction, testYhat[0,:,-1], 'r', label=\"LSTM prediction\")\n      plt.plot(len_prediction, NaiveForecastNormed[train_size+test_size:train_size+test_size+OUT_STEPS].AggregateLoad, 'g', label=\"Naive prediction\")\n        \n      plt.tight_layout()\n      sns.despine(top=True)\n      plt.subplots_adjust(left=0.07)\n      plt.ylabel('KWH per half hour (Normed)', size=15)\n      plt.xlabel('Time step', size=15)\n      plt.legend(fontsize=15)\n      plt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# install AutoGluon AutoML\n!pip install autogluon\nfrom autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AutoGluon specific data preparation\n# Split the time series data into train, test, and validation datasets\ntrain_size = int(len(autoData) * 0.7)  # 70% for training\nval_size = int(len(autoData) * 0.2)   # 20% for validation\ntest_size = len(autoData) - val_size - test_size  # Remaining for testing\n\nag_train_data = autoData[:train_size]\nag_val_data = autoData[train_size:train_size+val_size]\nag_test_data = autoData[train_size+val_size:]\n\nprint('ag_train_data:\\n', ag_train_data)\nprint('ag_val_data:\\n', ag_val_data)\nprint('ag_test_data\\n', ag_test_data)\n# print(ag_train_data.info())\n\n# AutoGluon requires an ItemID Column, so adding one...\nag_train_data['item_id'] = 'LoadSum'\nag_train_data = ag_train_data.astype({\"item_id\": str})\nag_val_data['item_id'] = 'LoadSum'\nag_val_data = ag_val_data.astype({\"item_id\": str})\nag_test_data['item_id'] = 'LoadSum'\nag_test_data = ag_test_data.astype({\"item_id\": str})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a quick look at the split datasets\nprint('ag_train_data\\n', ag_train_data)\nprint('ag_val_data\\n', ag_val_data)\nprint('ag_test_data\\n', ag_test_data)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load training data in to required AutoGluon proprietary data frame\n# print(ag_train_data.info())\nag_train_data_tsdf = TimeSeriesDataFrame.from_data_frame(\n    ag_train_data,\n    id_column=\"item_id\",\n    timestamp_column=\"DateTime\"\n)\nag_train_data_tsdf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load validation data in to required AutoGluon proprietary data frame, \"_tsdf\" suffix = time series data frame\nag_val_data_tsdf = TimeSeriesDataFrame.from_data_frame(\n    ag_val_data,\n    id_column=\"item_id\",\n    timestamp_column=\"DateTime\"\n)\nag_val_data_tsdf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load test data in to required AutoGluon proprietary data frame\n# print(ag_test_data.info())\nag_test_data_tsdf = TimeSeriesDataFrame.from_data_frame(\n    ag_test_data,\n    id_column=\"item_id\",\n    timestamp_column=\"DateTime\"\n)\nag_test_data_tsdf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# at \"high_quality\" level, training takes about 45 minutes...\n# training takes about 15 minutes for DeepAR\n# training takes about 21 minutes for TemporalFusionTransformer\n# training takes about 4 minutes for PatchTST\n# training takes about 4 minutes for PatchTST","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ag_predictor = TimeSeriesPredictor(\n    prediction_length=48,\n    path=\"autogluon-london-half-hourly\",\n    target=\"AggregateLoad\",\n    eval_metric=\"MASE\",\n)\n\nag_predictor.fit(\n    ag_train_data_tsdf,\n    presets=\"medium_quality\",\n    time_limit=6000,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The test score is computed using the last\n# prediction_length=48 timesteps of each time series in test_data\nag_predictor.leaderboard(ag_val_data_tsdf, silent=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate predictions\nag_predictions = ag_predictor.predict(ag_val_data_tsdf)\nag_predictions.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot prediction results, history and actual test data values\nplt.figure(figsize=(20, 3))\n\nitem_id = \"LoadSum\"\ny_past = ag_val_data_tsdf.loc[item_id][\"AggregateLoad\"]\ny_pred = ag_predictions.loc[item_id]\ny_val = ag_test_data_tsdf.loc[item_id][\"AggregateLoad\"]\n# y_val = ag_val_data_tsdf.loc[item_id][\"AggregateLoad\"]\n\nplt.plot(y_past[-100:], label=\"Past time series values\")\nplt.plot(y_pred[\"mean\"], label=\"Mean forecast\")\nplt.plot(y_val[:48], label=\"Future time series values\")\n\nplt.fill_between(\n    y_pred.index, y_pred[\"0.1\"], y_pred[\"0.9\"], color=\"red\", alpha=0.1, label=f\"10%-90% confidence interval\"\n)\nplt.legend();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}